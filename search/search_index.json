{"config": {"indexing": "full", "lang": ["en"], "min_search_length": 3, "prebuild_index": false, "separator": "[\\s\\-]+"}, "docs": [{"location": "", "text": "mkdocs-terminal # Terminal for MkDocs is a third party theme that brings the Terminal.css stylesheet to MkDocs documentation sites. In addition to simple, monospace styling, Terminal for MkDocs also provides: a built-in Search modal color palette options revision date display per-page or site-wide component hiding a flexible grid for displaying inline images section index pages and more Developer Info # Terminal for MkDocs on GitHub Terminal for MkDocs on PyPI Terminal for MkDocs Documentation MkDocs Documentation License # MIT License", "title": "Home"}, {"location": "#mkdocs-terminal", "text": "Terminal for MkDocs is a third party theme that brings the Terminal.css stylesheet to MkDocs documentation sites. In addition to simple, monospace styling, Terminal for MkDocs also provides: a built-in Search modal color palette options revision date display per-page or site-wide component hiding a flexible grid for displaying inline images section index pages and more", "title": "mkdocs-terminal"}, {"location": "#developer-info", "text": "Terminal for MkDocs on GitHub Terminal for MkDocs on PyPI Terminal for MkDocs Documentation MkDocs Documentation", "title": "Developer Info"}, {"location": "#license", "text": "MIT License", "title": "License"}, {"location": "about/license/", "text": "", "title": "License"}, {"location": "blog/", "text": "Crawl4AI Blog # Welcome to the Crawl4AI blog! Here you'll find detailed release notes, technical insights, and updates about the project. Whether you're looking for the latest improvements or want to dive deep into web crawling techniques, this is the place. Latest Release # Crawl4AI v0.5.0: Deep Crawling, Scalability, and a New CLI! # My dear friends and crawlers, there you go, this is the release of Crawl4AI v0.5.0! This release brings a wealth of new features, performance improvements, and a more streamlined developer experience. Here's a breakdown of what's new: Major New Features: Deep Crawling: Explore entire websites with configurable strategies (BFS, DFS, Best-First). Define custom filters and URL scoring for targeted crawls. Memory-Adaptive Dispatcher: Handle large-scale crawls with ease! Our new dispatcher dynamically adjusts concurrency based on available memory and includes built-in rate limiting. Multiple Crawler Strategies: Choose between the full-featured Playwright browser-based crawler or a new, much faster HTTP-only crawler for simpler tasks. Docker Deployment: Deploy Crawl4AI as a scalable, self-contained service with built-in API endpoints and optional JWT authentication. Command-Line Interface (CLI): Interact with Crawl4AI directly from your terminal. Crawl, configure, and extract data with simple commands. LLM Configuration ( LLMConfig ): A new, unified way to configure LLM providers (OpenAI, Anthropic, Ollama, etc.) for extraction, filtering, and schema generation. Simplifies API key management and switching between models. Minor Updates & Improvements: LXML Scraping Mode: Faster HTML parsing with LXMLWebScrapingStrategy . Proxy Rotation: Added ProxyRotationStrategy with a RoundRobinProxyStrategy implementation. PDF Processing: Extract text, images, and metadata from PDF files. URL Redirection Tracking: Automatically follows and records redirects. Robots.txt Compliance: Optionally respect website crawling rules. LLM-Powered Schema Generation: Automatically create extraction schemas using an LLM. LLMContentFilter : Generate high-quality, focused markdown using an LLM. Improved Error Handling & Stability: Numerous bug fixes and performance enhancements. Enhanced Documentation: Updated guides and examples. Breaking Changes & Migration: This release includes several breaking changes to improve the library's structure and consistency. Here's what you need to know: arun_many() Behavior: Now uses the MemoryAdaptiveDispatcher by default. The return type depends on the stream parameter in CrawlerRunConfig . Adjust code that relied on unbounded concurrency. max_depth Location: Moved to CrawlerRunConfig and now controls crawl depth . Deep Crawling Imports: Import DeepCrawlStrategy and related classes from crawl4ai.deep_crawling . BrowserContext API: Updated; the old get_context method is deprecated. Optional Model Fields: Many data model fields are now optional. Handle potential None values. ScrapingMode Enum: Replaced with strategy pattern ( WebScrapingStrategy , LXMLWebScrapingStrategy ). content_filter Parameter: Removed from CrawlerRunConfig . Use extraction strategies or markdown generators with filters. Removed Functionality: The synchronous WebCrawler , the old CLI, and docs management tools have been removed. Docker: Significant changes to deployment. See the Docker documentation . ssl_certificate.json : This file has been removed. Config : FastFilterChain has been replaced with FilterChain Deep-Crawl : DeepCrawlStrategy.arun now returns Union[CrawlResultT, List[CrawlResultT], AsyncGenerator[CrawlResultT, None]] Proxy : Removed synchronous WebCrawler support and related rate limiting configurations LLM Parameters: Use the new LLMConfig object instead of passing provider , api_token , base_url , and api_base directly to LLMExtractionStrategy and LLMContentFilter . In short: Update imports, adjust arun_many() usage, check for optional fields, and review the Docker deployment guide. License Change # Crawl4AI v0.5.0 updates the license to Apache 2.0 with a required attribution clause . This means you are free to use, modify, and distribute Crawl4AI (even commercially), but you must clearly attribute the project in any public use or distribution. See the updated LICENSE file for the full legal text and specific requirements. Get Started: Installation: pip install \"crawl4ai[all]\" (or use the Docker image) Documentation: https://docs.crawl4ai.com GitHub: https://github.com/unclecode/crawl4ai I'm very excited to see what you build with Crawl4AI v0.5.0! 0.4.2 - Configurable Crawlers, Session Management, and Smarter Screenshots # December 12, 2024 The 0.4.2 update brings massive improvements to configuration, making crawlers and browsers easier to manage with dedicated objects. You can now import/export local storage for seamless session management. Plus, long-page screenshots are faster and cleaner, and full-page PDF exports are now possible. Check out all the new features to make your crawling experience even smoother. Read full release notes \u2192 0.4.1 - Smarter Crawling with Lazy-Load Handling, Text-Only Mode, and More # December 8, 2024 This release brings major improvements to handling lazy-loaded images, a blazing-fast Text-Only Mode, full-page scanning for infinite scrolls, dynamic viewport adjustments, and session reuse for efficient crawling. If you're looking to improve speed, reliability, or handle dynamic content with ease, this update has you covered. Read full release notes \u2192 0.4.0 - Major Content Filtering Update # December 1, 2024 Introduced significant improvements to content filtering, multi-threaded environment handling, and user-agent generation. This release features the new PruningContentFilter, enhanced thread safety, and improved test coverage. Read full release notes \u2192 Project History # Curious about how Crawl4AI has evolved? Check out our complete changelog for a detailed history of all versions and updates. Stay Updated # Star us on GitHub Follow @unclecode on Twitter Join our community discussions on GitHub", "title": "Blog Home"}, {"location": "blog/#crawl4ai-blog", "text": "Welcome to the Crawl4AI blog! Here you'll find detailed release notes, technical insights, and updates about the project. Whether you're looking for the latest improvements or want to dive deep into web crawling techniques, this is the place.", "title": "Crawl4AI Blog"}, {"location": "blog/#latest-release", "text": "", "title": "Latest Release"}, {"location": "blog/#crawl4ai-v050-deep-crawling-scalability-and-a-new-cli", "text": "My dear friends and crawlers, there you go, this is the release of Crawl4AI v0.5.0! This release brings a wealth of new features, performance improvements, and a more streamlined developer experience. Here's a breakdown of what's new: Major New Features: Deep Crawling: Explore entire websites with configurable strategies (BFS, DFS, Best-First). Define custom filters and URL scoring for targeted crawls. Memory-Adaptive Dispatcher: Handle large-scale crawls with ease! Our new dispatcher dynamically adjusts concurrency based on available memory and includes built-in rate limiting. Multiple Crawler Strategies: Choose between the full-featured Playwright browser-based crawler or a new, much faster HTTP-only crawler for simpler tasks. Docker Deployment: Deploy Crawl4AI as a scalable, self-contained service with built-in API endpoints and optional JWT authentication. Command-Line Interface (CLI): Interact with Crawl4AI directly from your terminal. Crawl, configure, and extract data with simple commands. LLM Configuration ( LLMConfig ): A new, unified way to configure LLM providers (OpenAI, Anthropic, Ollama, etc.) for extraction, filtering, and schema generation. Simplifies API key management and switching between models. Minor Updates & Improvements: LXML Scraping Mode: Faster HTML parsing with LXMLWebScrapingStrategy . Proxy Rotation: Added ProxyRotationStrategy with a RoundRobinProxyStrategy implementation. PDF Processing: Extract text, images, and metadata from PDF files. URL Redirection Tracking: Automatically follows and records redirects. Robots.txt Compliance: Optionally respect website crawling rules. LLM-Powered Schema Generation: Automatically create extraction schemas using an LLM. LLMContentFilter : Generate high-quality, focused markdown using an LLM. Improved Error Handling & Stability: Numerous bug fixes and performance enhancements. Enhanced Documentation: Updated guides and examples. Breaking Changes & Migration: This release includes several breaking changes to improve the library's structure and consistency. Here's what you need to know: arun_many() Behavior: Now uses the MemoryAdaptiveDispatcher by default. The return type depends on the stream parameter in CrawlerRunConfig . Adjust code that relied on unbounded concurrency. max_depth Location: Moved to CrawlerRunConfig and now controls crawl depth . Deep Crawling Imports: Import DeepCrawlStrategy and related classes from crawl4ai.deep_crawling . BrowserContext API: Updated; the old get_context method is deprecated. Optional Model Fields: Many data model fields are now optional. Handle potential None values. ScrapingMode Enum: Replaced with strategy pattern ( WebScrapingStrategy , LXMLWebScrapingStrategy ). content_filter Parameter: Removed from CrawlerRunConfig . Use extraction strategies or markdown generators with filters. Removed Functionality: The synchronous WebCrawler , the old CLI, and docs management tools have been removed. Docker: Significant changes to deployment. See the Docker documentation . ssl_certificate.json : This file has been removed. Config : FastFilterChain has been replaced with FilterChain Deep-Crawl : DeepCrawlStrategy.arun now returns Union[CrawlResultT, List[CrawlResultT], AsyncGenerator[CrawlResultT, None]] Proxy : Removed synchronous WebCrawler support and related rate limiting configurations LLM Parameters: Use the new LLMConfig object instead of passing provider , api_token , base_url , and api_base directly to LLMExtractionStrategy and LLMContentFilter . In short: Update imports, adjust arun_many() usage, check for optional fields, and review the Docker deployment guide.", "title": "Crawl4AI v0.5.0: Deep Crawling, Scalability, and a New CLI!"}, {"location": "blog/#license-change", "text": "Crawl4AI v0.5.0 updates the license to Apache 2.0 with a required attribution clause . This means you are free to use, modify, and distribute Crawl4AI (even commercially), but you must clearly attribute the project in any public use or distribution. See the updated LICENSE file for the full legal text and specific requirements. Get Started: Installation: pip install \"crawl4ai[all]\" (or use the Docker image) Documentation: https://docs.crawl4ai.com GitHub: https://github.com/unclecode/crawl4ai I'm very excited to see what you build with Crawl4AI v0.5.0!", "title": "License Change"}, {"location": "blog/#042-configurable-crawlers-session-management-and-smarter-screenshots", "text": "December 12, 2024 The 0.4.2 update brings massive improvements to configuration, making crawlers and browsers easier to manage with dedicated objects. You can now import/export local storage for seamless session management. Plus, long-page screenshots are faster and cleaner, and full-page PDF exports are now possible. Check out all the new features to make your crawling experience even smoother. Read full release notes \u2192", "title": "0.4.2 - Configurable Crawlers, Session Management, and Smarter Screenshots"}, {"location": "blog/#041-smarter-crawling-with-lazy-load-handling-text-only-mode-and-more", "text": "December 8, 2024 This release brings major improvements to handling lazy-loaded images, a blazing-fast Text-Only Mode, full-page scanning for infinite scrolls, dynamic viewport adjustments, and session reuse for efficient crawling. If you're looking to improve speed, reliability, or handle dynamic content with ease, this update has you covered. Read full release notes \u2192", "title": "0.4.1 - Smarter Crawling with Lazy-Load Handling, Text-Only Mode, and More"}, {"location": "blog/#040-major-content-filtering-update", "text": "December 1, 2024 Introduced significant improvements to content filtering, multi-threaded environment handling, and user-agent generation. This release features the new PruningContentFilter, enhanced thread safety, and improved test coverage. Read full release notes \u2192", "title": "0.4.0 - Major Content Filtering Update"}, {"location": "blog/#project-history", "text": "Curious about how Crawl4AI has evolved? Check out our complete changelog for a detailed history of all versions and updates.", "title": "Project History"}, {"location": "blog/#stay-updated", "text": "Star us on GitHub Follow @unclecode on Twitter Join our community discussions on GitHub", "title": "Stay Updated"}, {"location": "blog/releases/releases_v0.0.1/", "text": "Release Summary for Version 0.4.0 (December 1, 2024) # Overview # The 0.4.0 release introduces significant improvements to content filtering, multi-threaded environment handling, user-agent generation, and test coverage. Key highlights include the introduction of the PruningContentFilter, designed to automatically identify and extract the most valuable parts of an HTML document, as well as enhancements to the BM25ContentFilter to extend its versatility and effectiveness. Major Features and Enhancements # 1. PruningContentFilter # Introduced a new unsupervised content filtering strategy that scores and prunes less relevant nodes in an HTML document based on metrics like text and link density. Focuses on retaining the most valuable parts of the content, making it highly effective for extracting relevant information from complex web pages. Fully documented with updated README and expanded user guides. 2. User-Agent Generator # Added a user-agent generator utility that resolves compatibility issues and supports customizable user-agent strings. By default, the generator randomizes user agents for each request, adding diversity, but users can customize it for tailored scenarios. 3. Enhanced Thread Safety # Improved handling of multi-threaded environments by adding better thread locks for parallel processing, ensuring consistency and stability when running multiple threads. 4. Extended Content Filtering Strategies # Users now have access to both the PruningContentFilter for unsupervised extraction and the BM25ContentFilter for supervised filtering based on user queries. Enhanced BM25ContentFilter with improved capabilities to process page titles, meta tags, and descriptions, allowing for more effective classification and clustering of text chunks. 5. Documentation Updates # Updated examples and tutorials to promote the use of the PruningContentFilter alongside the BM25ContentFilter, providing clear instructions for selecting the appropriate filter for each use case. 6. Unit Test Enhancements # Added unit tests for PruningContentFilter to ensure accuracy and reliability. Enhanced BM25ContentFilter tests to cover additional edge cases and performance metrics, particularly for malformed HTML inputs. Revised Change Logs for Version 0.4.0 # PruningContentFilter (Dec 01, 2024) # Introduced the PruningContentFilter to optimize content extraction by pruning less relevant HTML nodes. Affected Files: crawl4ai/content_filter_strategy.py : Added a scoring-based pruning algorithm. README.md : Updated to include PruningContentFilter usage. docs/md_v2/basic/content_filtering.md : Expanded user documentation, detailing the use and benefits of PruningContentFilter. Unit Tests for PruningContentFilter (Dec 01, 2024) # Added comprehensive unit tests for PruningContentFilter to ensure correctness and efficiency. Affected Files: tests/async/test_content_filter_prune.py : Created tests covering different pruning scenarios to ensure stability and correctness. Enhanced BM25ContentFilter Tests (Dec 01, 2024) # Expanded tests to cover additional extraction scenarios and performance metrics, improving robustness. Affected Files: tests/async/test_content_filter_bm25.py : Added tests for edge cases, including malformed HTML inputs. Documentation and Example Updates (Dec 01, 2024) # Revised examples to illustrate the use of PruningContentFilter alongside existing content filtering methods. Affected Files: docs/examples/quickstart_async.py : Enhanced example clarity and usability for new users. Experimental Features # The PruningContentFilter is still under experimental development, and we continue to gather feedback for further refinements. Conclusion # This release significantly enhances the content extraction capabilities of Crawl4ai with the introduction of the PruningContentFilter, improved supervised filtering with BM25ContentFilter, and robust multi-threaded handling. Additionally, the user-agent generator provides much-needed versatility, resolving compatibility issues faced by many users. Users are encouraged to experiment with the new content filtering methods to determine which best suits their needs.", "title": "Release Summary for Version 0.4.0 (December 1, 2024)"}, {"location": "blog/releases/releases_v0.0.1/#release-summary-for-version-040-december-1-2024", "text": "", "title": "Release Summary for Version 0.4.0 (December 1, 2024)"}, {"location": "blog/releases/releases_v0.0.1/#overview", "text": "The 0.4.0 release introduces significant improvements to content filtering, multi-threaded environment handling, user-agent generation, and test coverage. Key highlights include the introduction of the PruningContentFilter, designed to automatically identify and extract the most valuable parts of an HTML document, as well as enhancements to the BM25ContentFilter to extend its versatility and effectiveness.", "title": "Overview"}, {"location": "blog/releases/releases_v0.0.1/#major-features-and-enhancements", "text": "", "title": "Major Features and Enhancements"}, {"location": "blog/releases/releases_v0.0.1/#1-pruningcontentfilter", "text": "Introduced a new unsupervised content filtering strategy that scores and prunes less relevant nodes in an HTML document based on metrics like text and link density. Focuses on retaining the most valuable parts of the content, making it highly effective for extracting relevant information from complex web pages. Fully documented with updated README and expanded user guides.", "title": "1. PruningContentFilter"}, {"location": "blog/releases/releases_v0.0.1/#2-user-agent-generator", "text": "Added a user-agent generator utility that resolves compatibility issues and supports customizable user-agent strings. By default, the generator randomizes user agents for each request, adding diversity, but users can customize it for tailored scenarios.", "title": "2. User-Agent Generator"}, {"location": "blog/releases/releases_v0.0.1/#3-enhanced-thread-safety", "text": "Improved handling of multi-threaded environments by adding better thread locks for parallel processing, ensuring consistency and stability when running multiple threads.", "title": "3. Enhanced Thread Safety"}, {"location": "blog/releases/releases_v0.0.1/#4-extended-content-filtering-strategies", "text": "Users now have access to both the PruningContentFilter for unsupervised extraction and the BM25ContentFilter for supervised filtering based on user queries. Enhanced BM25ContentFilter with improved capabilities to process page titles, meta tags, and descriptions, allowing for more effective classification and clustering of text chunks.", "title": "4. Extended Content Filtering Strategies"}, {"location": "blog/releases/releases_v0.0.1/#5-documentation-updates", "text": "Updated examples and tutorials to promote the use of the PruningContentFilter alongside the BM25ContentFilter, providing clear instructions for selecting the appropriate filter for each use case.", "title": "5. Documentation Updates"}, {"location": "blog/releases/releases_v0.0.1/#6-unit-test-enhancements", "text": "Added unit tests for PruningContentFilter to ensure accuracy and reliability. Enhanced BM25ContentFilter tests to cover additional edge cases and performance metrics, particularly for malformed HTML inputs.", "title": "6. Unit Test Enhancements"}, {"location": "blog/releases/releases_v0.0.1/#revised-change-logs-for-version-040", "text": "", "title": "Revised Change Logs for Version 0.4.0"}, {"location": "blog/releases/releases_v0.0.1/#pruningcontentfilter-dec-01-2024", "text": "Introduced the PruningContentFilter to optimize content extraction by pruning less relevant HTML nodes. Affected Files: crawl4ai/content_filter_strategy.py : Added a scoring-based pruning algorithm. README.md : Updated to include PruningContentFilter usage. docs/md_v2/basic/content_filtering.md : Expanded user documentation, detailing the use and benefits of PruningContentFilter.", "title": "PruningContentFilter (Dec 01, 2024)"}, {"location": "blog/releases/releases_v0.0.1/#unit-tests-for-pruningcontentfilter-dec-01-2024", "text": "Added comprehensive unit tests for PruningContentFilter to ensure correctness and efficiency. Affected Files: tests/async/test_content_filter_prune.py : Created tests covering different pruning scenarios to ensure stability and correctness.", "title": "Unit Tests for PruningContentFilter (Dec 01, 2024)"}, {"location": "blog/releases/releases_v0.0.1/#enhanced-bm25contentfilter-tests-dec-01-2024", "text": "Expanded tests to cover additional extraction scenarios and performance metrics, improving robustness. Affected Files: tests/async/test_content_filter_bm25.py : Added tests for edge cases, including malformed HTML inputs.", "title": "Enhanced BM25ContentFilter Tests (Dec 01, 2024)"}, {"location": "blog/releases/releases_v0.0.1/#documentation-and-example-updates-dec-01-2024", "text": "Revised examples to illustrate the use of PruningContentFilter alongside existing content filtering methods. Affected Files: docs/examples/quickstart_async.py : Enhanced example clarity and usability for new users.", "title": "Documentation and Example Updates (Dec 01, 2024)"}, {"location": "blog/releases/releases_v0.0.1/#experimental-features", "text": "The PruningContentFilter is still under experimental development, and we continue to gather feedback for further refinements.", "title": "Experimental Features"}, {"location": "blog/releases/releases_v0.0.1/#conclusion", "text": "This release significantly enhances the content extraction capabilities of Crawl4ai with the introduction of the PruningContentFilter, improved supervised filtering with BM25ContentFilter, and robust multi-threaded handling. Additionally, the user-agent generator provides much-needed versatility, resolving compatibility issues faced by many users. Users are encouraged to experiment with the new content filtering methods to determine which best suits their needs.", "title": "Conclusion"}, {"location": "quick_start/quick_start/", "text": "Getting Started with Crawl4AI # Welcome to Crawl4AI , an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, you\u2019ll: Run your first crawl using minimal configuration. Generate Markdown output (and learn how it\u2019s influenced by content filters). Experiment with a simple CSS-based extraction strategy. See a glimpse of LLM-based extraction (including open-source and closed-source model options). Crawl a dynamic page that loads content via JavaScript. 1. Introduction # Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler . Configurable browser and run settings via BrowserConfig and CrawlerRunConfig . Automatic HTML-to-Markdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLM-based or \u201ctraditional\u201d CSS/XPath-based). By the end of this guide, you\u2019ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses \u201cLoad More\u201d buttons or JavaScript updates. 2. Your First Crawl # Here\u2019s a minimal Python script that creates an AsyncWebCrawler , fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main (): async with AsyncWebCrawler() as crawler: result = await crawler . arun( \"https://example.com\" ) print (result . markdown[: 300 ]) # Print first 300 chars if __name__ == \"__main__\" : asyncio . run(main()) What\u2019s happening? - AsyncWebCrawler launches a headless browser (Chromium by default). - It fetches https://example.com . - Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl! 3. Basic Configuration (Light Introduction) # Crawl4AI\u2019s crawler can be heavily customized using two main classes: 1.\u2000 BrowserConfig : Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2.\u2000 CrawlerRunConfig : Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main (): browser_conf = BrowserConfig(headless = True ) # or False to see the browser run_conf = CrawlerRunConfig( cache_mode = CacheMode . BYPASS ) async with AsyncWebCrawler(config = browser_conf) as crawler: result = await crawler . arun( url = \"https://example.com\" , config = run_conf ) print (result . markdown) if __name__ == \"__main__\" : asyncio . run(main()) IMPORTANT: By default cache mode is set to CacheMode.ENABLED . So to have fresh content, you need to set it to CacheMode.BYPASS We\u2019ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling. 4. Generating Markdown Output # By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter . result.markdown : The direct HTML-to-Markdown conversion. result.markdown.fit_markdown : The same content after applying any configured content filter (e.g., PruningContentFilter ). Example: Using a Filter with DefaultMarkdownGenerator # from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.content_filter_strategy import PruningContentFilter from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator md_generator = DefaultMarkdownGenerator( content_filter = PruningContentFilter(threshold =0.4 , threshold_type = \"fixed\" ) ) config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, markdown_generator = md_generator ) async with AsyncWebCrawler() as crawler: result = await crawler . arun( \"https://news.ycombinator.com\" , config = config) print ( \"Raw Markdown length:\" , len (result . markdown . raw_markdown)) print ( \"Fit Markdown length:\" , len (result . markdown . fit_markdown)) Note : If you do not specify a content filter or markdown generator, you\u2019ll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. We\u2019ll dive deeper into these strategies in a dedicated Markdown Generation tutorial. 5. Simple Data Extraction (CSS-based) # Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example: New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions: from crawl4ai.extraction_strategy import JsonCssExtractionStrategy from crawl4ai.types import LLMConfig # Generate a schema (one-time cost) html = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\" # Using OpenAI (requires API token) schema = JsonCssExtractionStrategy . generate_schema( html, llm_config = LLMConfig(provider = \"openai/gpt-4o\" ,api_token = \"your-openai-token\" ) # Required for OpenAI ) # Or using Ollama (open source, no token needed) schema = JsonCssExtractionStrategy . generate_schema( html, llm_config = LLMConfig(provider = \"ollama/llama3.3\" , api_token = None ) # Not needed for Ollama ) # Use the schema for fast, repeated extractions strategy = JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see No-LLM Extraction Strategies . Here's a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def main (): schema = { \"name\" : \"Example Items\" , \"baseSelector\" : \"div.item\" , \"fields\" : [ { \"name\" : \"title\" , \"selector\" : \"h2\" , \"type\" : \"text\" }, { \"name\" : \"link\" , \"selector\" : \"a\" , \"type\" : \"attribute\" , \"attribute\" : \"href\" } ] } raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\" async with AsyncWebCrawler() as crawler: result = await crawler . arun( url = \"raw://\" + raw_html, config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, extraction_strategy = JsonCssExtractionStrategy(schema) ) ) # The JSON output is stored in 'extracted_content' data = json . loads(result . extracted_content) print (data) if __name__ == \"__main__\" : asyncio . run(main()) Why is this helpful? - Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store. Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw:// . 6. Simple Data Extraction (LLM-based) # For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports open-source or closed-source providers: Open-Source Models (e.g., ollama/llama3.3 , no_token ) OpenAI Models (e.g., openai/gpt-4 , requires api_token ) Or any provider supported by the underlying library Below is an example using open-source style (no token) and closed-source: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig from crawl4ai.extraction_strategy import LLMExtractionStrategy class OpenAIModelFee (BaseModel): model_name: str = Field( ... , description = \"Name of the OpenAI model.\" ) input_fee: str = Field( ... , description = \"Fee for input token for the OpenAI model.\" ) output_fee: str = Field( ... , description = \"Fee for output token for the OpenAI model.\" ) async def extract_structured_data_using_llm ( provider: str , api_token: str = None , extra_headers: Dict[ str , str ] = None ): print ( f\" \\n --- Extracting Structured Data with { provider } ---\" ) if api_token is None and provider != \"ollama\" : print ( f\"API token is required for { provider } . Skipping this example.\" ) return browser_config = BrowserConfig(headless = True ) extra_args = { \"temperature\" : 0 , \"top_p\" : 0.9 , \"max_tokens\" : 2000 } if extra_headers: extra_args[ \"extra_headers\" ] = extra_headers crawler_config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, word_count_threshold =1 , page_timeout =80000 , extraction_strategy = LLMExtractionStrategy( llm_config = LLMConfig(provider = provider,api_token = api_token), schema = OpenAIModelFee . model_json_schema(), extraction_type = \"schema\" , instruction = \"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content.\"\"\" , extra_args = extra_args, ), ) async with AsyncWebCrawler(config = browser_config) as crawler: result = await crawler . arun( url = \"https://openai.com/api/pricing/\" , config = crawler_config ) print (result . extracted_content) if __name__ == \"__main__\" : asyncio . run( extract_structured_data_using_llm( provider = \"openai/gpt-4o\" , api_token = os . getenv( \"OPENAI_API_KEY\" ) ) ) What\u2019s happening? - We define a Pydantic schema ( PricingInfo ) describing the fields we want. - The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON. - Depending on the provider and api_token , you can use local models or a remote API. 7. Multi-URL Concurrency (Preview) # If you need to crawl multiple URLs in parallel , you can use arun_many() . By default, Crawl4AI employs a MemoryAdaptiveDispatcher , automatically adjusting concurrency based on system resources. Here\u2019s a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quick_parallel_example (): urls = [ \"https://example.com/page1\" , \"https://example.com/page2\" , \"https://example.com/page3\" ] run_conf = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, stream = True # Enable streaming mode ) async with AsyncWebCrawler() as crawler: # Stream results as they complete async for result in await crawler . arun_many(urls, config = run_conf): if result . success: print ( f\"[OK] { result . url } , length: { len (result . markdown . raw_markdown) } \" ) else : print ( f\"[ERROR] { result . url } => { result . error_message } \" ) # Or get all results at once (default behavior) run_conf = run_conf . clone(stream = False ) results = await crawler . arun_many(urls, config = run_conf) for res in results: if res . success: print ( f\"[OK] { res . url } , length: { len (res . markdown . raw_markdown) } \" ) else : print ( f\"[ERROR] { res . url } => { res . error_message } \" ) if __name__ == \"__main__\" : asyncio . run(quick_parallel_example()) The example above shows two ways to handle multiple URLs: 1. Streaming mode ( stream=True ): Process results as they become available using async for 2. Batch mode ( stream=False ): Wait for all results to complete For more advanced concurrency (e.g., a semaphore-based approach, adaptive memory usage throttling , or customized rate limiting), see Advanced Multi-URL Crawling . 8. Dynamic Content Example # Some sites require multiple \u201cpage clicks\u201d or dynamic JavaScript updates. Below is an example showing how to click a \u201cNext Page\u201d button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig : import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def extract_structured_data_using_css_extractor (): print ( \" \\n --- Using JsonCssExtractionStrategy for Fast Structured Output ---\" ) schema = { \"name\" : \"KidoCode Courses\" , \"baseSelector\" : \"section.charge-methodology .w-tab-content > div\" , \"fields\" : [ { \"name\" : \"section_title\" , \"selector\" : \"h3.heading-50\" , \"type\" : \"text\" , }, { \"name\" : \"section_description\" , \"selector\" : \".charge-content\" , \"type\" : \"text\" , }, { \"name\" : \"course_name\" , \"selector\" : \".text-block-93\" , \"type\" : \"text\" , }, { \"name\" : \"course_description\" , \"selector\" : \".course-content-text\" , \"type\" : \"text\" , }, { \"name\" : \"course_icon\" , \"selector\" : \".image-92\" , \"type\" : \"attribute\" , \"attribute\" : \"src\" , }, ], } browser_config = BrowserConfig(headless = True , java_script_enabled = True ) js_click_tabs = \"\"\" (async () => { const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\"); for(let tab of tabs) { tab.scrollIntoView(); tab.click(); await new Promise(r => setTimeout(r, 500)); } })(); \"\"\" crawler_config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, extraction_strategy = JsonCssExtractionStrategy(schema), js_code = [js_click_tabs], ) async with AsyncWebCrawler(config = browser_config) as crawler: result = await crawler . arun( url = \"https://www.kidocode.com/degrees/technology\" , config = crawler_config ) companies = json . loads(result . extracted_content) print ( f\"Successfully extracted { len (companies) } companies\" ) print (json . dumps(companies[ 0 ], indent =2 )) async def main (): await extract_structured_data_using_css_extractor() if __name__ == \"__main__\" : asyncio . run(main()) Key Points : BrowserConfig(headless=False) : We want to watch it click \u201cNext Page.\u201d CrawlerRunConfig(...) : We specify the extraction strategy, pass session_id to reuse the same page. js_code and wait_for are used for subsequent pages ( page > 0 ) to click the \u201cNext\u201d button and wait for new commits to load. js_only=True indicates we\u2019re not re-navigating but continuing the existing session. Finally, we call kill_session() to clean up the page and browser session. 9. Next Steps # Congratulations! You have: Performed a basic crawl and printed Markdown. Used content filters with a markdown generator. Extracted JSON via CSS or LLM strategies. Handled dynamic pages with JavaScript triggers. If you\u2019re ready for more, check out: Installation : A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks & Auth : Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment : Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management : Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling!", "title": "Quick Start"}, {"location": "quick_start/quick_start/#getting-started-with-crawl4ai", "text": "Welcome to Crawl4AI , an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, you\u2019ll: Run your first crawl using minimal configuration. Generate Markdown output (and learn how it\u2019s influenced by content filters). Experiment with a simple CSS-based extraction strategy. See a glimpse of LLM-based extraction (including open-source and closed-source model options). Crawl a dynamic page that loads content via JavaScript.", "title": "Getting Started with Crawl4AI"}, {"location": "quick_start/quick_start/#1-introduction", "text": "Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler . Configurable browser and run settings via BrowserConfig and CrawlerRunConfig . Automatic HTML-to-Markdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLM-based or \u201ctraditional\u201d CSS/XPath-based). By the end of this guide, you\u2019ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses \u201cLoad More\u201d buttons or JavaScript updates.", "title": "1. Introduction"}, {"location": "quick_start/quick_start/#2-your-first-crawl", "text": "Here\u2019s a minimal Python script that creates an AsyncWebCrawler , fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main (): async with AsyncWebCrawler() as crawler: result = await crawler . arun( \"https://example.com\" ) print (result . markdown[: 300 ]) # Print first 300 chars if __name__ == \"__main__\" : asyncio . run(main()) What\u2019s happening? - AsyncWebCrawler launches a headless browser (Chromium by default). - It fetches https://example.com . - Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl!", "title": "2. Your First Crawl"}, {"location": "quick_start/quick_start/#3-basic-configuration-light-introduction", "text": "Crawl4AI\u2019s crawler can be heavily customized using two main classes: 1.\u2000 BrowserConfig : Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2.\u2000 CrawlerRunConfig : Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main (): browser_conf = BrowserConfig(headless = True ) # or False to see the browser run_conf = CrawlerRunConfig( cache_mode = CacheMode . BYPASS ) async with AsyncWebCrawler(config = browser_conf) as crawler: result = await crawler . arun( url = \"https://example.com\" , config = run_conf ) print (result . markdown) if __name__ == \"__main__\" : asyncio . run(main()) IMPORTANT: By default cache mode is set to CacheMode.ENABLED . So to have fresh content, you need to set it to CacheMode.BYPASS We\u2019ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling.", "title": "3. Basic Configuration (Light Introduction)"}, {"location": "quick_start/quick_start/#4-generating-markdown-output", "text": "By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter . result.markdown : The direct HTML-to-Markdown conversion. result.markdown.fit_markdown : The same content after applying any configured content filter (e.g., PruningContentFilter ).", "title": "4. Generating Markdown Output"}, {"location": "quick_start/quick_start/#example-using-a-filter-with-defaultmarkdowngenerator", "text": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.content_filter_strategy import PruningContentFilter from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator md_generator = DefaultMarkdownGenerator( content_filter = PruningContentFilter(threshold =0.4 , threshold_type = \"fixed\" ) ) config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, markdown_generator = md_generator ) async with AsyncWebCrawler() as crawler: result = await crawler . arun( \"https://news.ycombinator.com\" , config = config) print ( \"Raw Markdown length:\" , len (result . markdown . raw_markdown)) print ( \"Fit Markdown length:\" , len (result . markdown . fit_markdown)) Note : If you do not specify a content filter or markdown generator, you\u2019ll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. We\u2019ll dive deeper into these strategies in a dedicated Markdown Generation tutorial.", "title": "Example: Using a Filter with DefaultMarkdownGenerator"}, {"location": "quick_start/quick_start/#5-simple-data-extraction-css-based", "text": "Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example: New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions: from crawl4ai.extraction_strategy import JsonCssExtractionStrategy from crawl4ai.types import LLMConfig # Generate a schema (one-time cost) html = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\" # Using OpenAI (requires API token) schema = JsonCssExtractionStrategy . generate_schema( html, llm_config = LLMConfig(provider = \"openai/gpt-4o\" ,api_token = \"your-openai-token\" ) # Required for OpenAI ) # Or using Ollama (open source, no token needed) schema = JsonCssExtractionStrategy . generate_schema( html, llm_config = LLMConfig(provider = \"ollama/llama3.3\" , api_token = None ) # Not needed for Ollama ) # Use the schema for fast, repeated extractions strategy = JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see No-LLM Extraction Strategies . Here's a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def main (): schema = { \"name\" : \"Example Items\" , \"baseSelector\" : \"div.item\" , \"fields\" : [ { \"name\" : \"title\" , \"selector\" : \"h2\" , \"type\" : \"text\" }, { \"name\" : \"link\" , \"selector\" : \"a\" , \"type\" : \"attribute\" , \"attribute\" : \"href\" } ] } raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\" async with AsyncWebCrawler() as crawler: result = await crawler . arun( url = \"raw://\" + raw_html, config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, extraction_strategy = JsonCssExtractionStrategy(schema) ) ) # The JSON output is stored in 'extracted_content' data = json . loads(result . extracted_content) print (data) if __name__ == \"__main__\" : asyncio . run(main()) Why is this helpful? - Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store. Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw:// .", "title": "5. Simple Data Extraction (CSS-based)"}, {"location": "quick_start/quick_start/#6-simple-data-extraction-llm-based", "text": "For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports open-source or closed-source providers: Open-Source Models (e.g., ollama/llama3.3 , no_token ) OpenAI Models (e.g., openai/gpt-4 , requires api_token ) Or any provider supported by the underlying library Below is an example using open-source style (no token) and closed-source: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig from crawl4ai.extraction_strategy import LLMExtractionStrategy class OpenAIModelFee (BaseModel): model_name: str = Field( ... , description = \"Name of the OpenAI model.\" ) input_fee: str = Field( ... , description = \"Fee for input token for the OpenAI model.\" ) output_fee: str = Field( ... , description = \"Fee for output token for the OpenAI model.\" ) async def extract_structured_data_using_llm ( provider: str , api_token: str = None , extra_headers: Dict[ str , str ] = None ): print ( f\" \\n --- Extracting Structured Data with { provider } ---\" ) if api_token is None and provider != \"ollama\" : print ( f\"API token is required for { provider } . Skipping this example.\" ) return browser_config = BrowserConfig(headless = True ) extra_args = { \"temperature\" : 0 , \"top_p\" : 0.9 , \"max_tokens\" : 2000 } if extra_headers: extra_args[ \"extra_headers\" ] = extra_headers crawler_config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, word_count_threshold =1 , page_timeout =80000 , extraction_strategy = LLMExtractionStrategy( llm_config = LLMConfig(provider = provider,api_token = api_token), schema = OpenAIModelFee . model_json_schema(), extraction_type = \"schema\" , instruction = \"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content.\"\"\" , extra_args = extra_args, ), ) async with AsyncWebCrawler(config = browser_config) as crawler: result = await crawler . arun( url = \"https://openai.com/api/pricing/\" , config = crawler_config ) print (result . extracted_content) if __name__ == \"__main__\" : asyncio . run( extract_structured_data_using_llm( provider = \"openai/gpt-4o\" , api_token = os . getenv( \"OPENAI_API_KEY\" ) ) ) What\u2019s happening? - We define a Pydantic schema ( PricingInfo ) describing the fields we want. - The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON. - Depending on the provider and api_token , you can use local models or a remote API.", "title": "6. Simple Data Extraction (LLM-based)"}, {"location": "quick_start/quick_start/#7-multi-url-concurrency-preview", "text": "If you need to crawl multiple URLs in parallel , you can use arun_many() . By default, Crawl4AI employs a MemoryAdaptiveDispatcher , automatically adjusting concurrency based on system resources. Here\u2019s a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quick_parallel_example (): urls = [ \"https://example.com/page1\" , \"https://example.com/page2\" , \"https://example.com/page3\" ] run_conf = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, stream = True # Enable streaming mode ) async with AsyncWebCrawler() as crawler: # Stream results as they complete async for result in await crawler . arun_many(urls, config = run_conf): if result . success: print ( f\"[OK] { result . url } , length: { len (result . markdown . raw_markdown) } \" ) else : print ( f\"[ERROR] { result . url } => { result . error_message } \" ) # Or get all results at once (default behavior) run_conf = run_conf . clone(stream = False ) results = await crawler . arun_many(urls, config = run_conf) for res in results: if res . success: print ( f\"[OK] { res . url } , length: { len (res . markdown . raw_markdown) } \" ) else : print ( f\"[ERROR] { res . url } => { res . error_message } \" ) if __name__ == \"__main__\" : asyncio . run(quick_parallel_example()) The example above shows two ways to handle multiple URLs: 1. Streaming mode ( stream=True ): Process results as they become available using async for 2. Batch mode ( stream=False ): Wait for all results to complete For more advanced concurrency (e.g., a semaphore-based approach, adaptive memory usage throttling , or customized rate limiting), see Advanced Multi-URL Crawling .", "title": "7. Multi-URL Concurrency (Preview)"}, {"location": "quick_start/quick_start/#8-dynamic-content-example", "text": "Some sites require multiple \u201cpage clicks\u201d or dynamic JavaScript updates. Below is an example showing how to click a \u201cNext Page\u201d button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig : import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def extract_structured_data_using_css_extractor (): print ( \" \\n --- Using JsonCssExtractionStrategy for Fast Structured Output ---\" ) schema = { \"name\" : \"KidoCode Courses\" , \"baseSelector\" : \"section.charge-methodology .w-tab-content > div\" , \"fields\" : [ { \"name\" : \"section_title\" , \"selector\" : \"h3.heading-50\" , \"type\" : \"text\" , }, { \"name\" : \"section_description\" , \"selector\" : \".charge-content\" , \"type\" : \"text\" , }, { \"name\" : \"course_name\" , \"selector\" : \".text-block-93\" , \"type\" : \"text\" , }, { \"name\" : \"course_description\" , \"selector\" : \".course-content-text\" , \"type\" : \"text\" , }, { \"name\" : \"course_icon\" , \"selector\" : \".image-92\" , \"type\" : \"attribute\" , \"attribute\" : \"src\" , }, ], } browser_config = BrowserConfig(headless = True , java_script_enabled = True ) js_click_tabs = \"\"\" (async () => { const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\"); for(let tab of tabs) { tab.scrollIntoView(); tab.click(); await new Promise(r => setTimeout(r, 500)); } })(); \"\"\" crawler_config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, extraction_strategy = JsonCssExtractionStrategy(schema), js_code = [js_click_tabs], ) async with AsyncWebCrawler(config = browser_config) as crawler: result = await crawler . arun( url = \"https://www.kidocode.com/degrees/technology\" , config = crawler_config ) companies = json . loads(result . extracted_content) print ( f\"Successfully extracted { len (companies) } companies\" ) print (json . dumps(companies[ 0 ], indent =2 )) async def main (): await extract_structured_data_using_css_extractor() if __name__ == \"__main__\" : asyncio . run(main()) Key Points : BrowserConfig(headless=False) : We want to watch it click \u201cNext Page.\u201d CrawlerRunConfig(...) : We specify the extraction strategy, pass session_id to reuse the same page. js_code and wait_for are used for subsequent pages ( page > 0 ) to click the \u201cNext\u201d button and wait for new commits to load. js_only=True indicates we\u2019re not re-navigating but continuing the existing session. Finally, we call kill_session() to clean up the page and browser session.", "title": "8. Dynamic Content Example"}, {"location": "quick_start/quick_start/#9-next-steps", "text": "Congratulations! You have: Performed a basic crawl and printed Markdown. Used content filters with a markdown generator. Extracted JSON via CSS or LLM strategies. Handled dynamic pages with JavaScript triggers. If you\u2019re ready for more, check out: Installation : A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks & Auth : Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment : Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management : Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling!", "title": "9. Next Steps"}, {"location": "setup/docker-deployment/", "text": "Docker Deployment # Crawl4AI provides official Docker images for easy deployment and scalability. This guide covers installation, configuration, and usage of Crawl4AI in Docker environments. Quick Start \ud83d\ude80 # Pull and run the basic version: # Basic run without security docker pull unclecode/crawl4ai:basic docker run -p 11235 :11235 unclecode/crawl4ai:basic # Run with API security enabled docker run -p 11235 :11235 -e CRAWL4AI_API_TOKEN = your_secret_token unclecode/crawl4ai:basic Running with Docker Compose \ud83d\udc33 # Use Docker Compose (From Local Dockerfile or Docker Hub) # Crawl4AI provides flexibility to use Docker Compose for managing your containerized services. You can either build the image locally from the provided Dockerfile or use the pre-built image from Docker Hub. Option 1: Using Docker Compose to Build Locally # If you want to build the image locally, use the provided docker-compose.local.yml file. docker-compose -f docker-compose.local.yml up -d This will: 1. Build the Docker image from the provided Dockerfile . 2. Start the container and expose it on http://localhost:11235 . Option 2: Using Docker Compose with Pre-Built Image from Hub # If you prefer using the pre-built image on Docker Hub, use the docker-compose.hub.yml file. docker-compose -f docker-compose.hub.yml up -d This will: 1. Pull the pre-built image unclecode/crawl4ai:basic (or all , depending on your configuration). 2. Start the container and expose it on http://localhost:11235 . Stopping the Running Services # To stop the services started via Docker Compose, you can use: docker-compose -f docker-compose.local.yml down # OR docker-compose -f docker-compose.hub.yml down If the containers don\u2019t stop and the application is still running, check the running containers: docker ps Find the CONTAINER ID of the running service and stop it forcefully: docker stop <CONTAINER_ID> Debugging with Docker Compose # Check Logs : To view the container logs: docker-compose -f docker-compose.local.yml logs -f Remove Orphaned Containers : If the service is still running unexpectedly: docker-compose -f docker-compose.local.yml down --remove-orphans Manually Remove Network : If the network is still in use: docker network ls docker network rm crawl4ai_default Why Use Docker Compose? # Docker Compose is the recommended way to deploy Crawl4AI because: 1. It simplifies multi-container setups. 2. Allows you to define environment variables, resources, and ports in a single file. 3. Makes it easier to switch between local development and production-ready images. For example, your docker-compose.yml could include API keys, token settings, and memory limits, making deployment quick and consistent. API Security \ud83d\udd12 # Understanding CRAWL4AI_API_TOKEN # The CRAWL4AI_API_TOKEN provides optional security for your Crawl4AI instance: If CRAWL4AI_API_TOKEN is set: All API endpoints (except /health ) require authentication If CRAWL4AI_API_TOKEN is not set: The API is publicly accessible # Secured Instance docker run -p 11235 :11235 -e CRAWL4AI_API_TOKEN = your_secret_token unclecode/crawl4ai:all # Unsecured Instance docker run -p 11235 :11235 unclecode/crawl4ai:all Making API Calls # For secured instances, include the token in all requests: import requests # Setup headers if token is being used api_token = \"your_secret_token\" # Same token set in CRAWL4AI_API_TOKEN headers = { \"Authorization\" : f\"Bearer { api_token } \" } if api_token else {} # Making authenticated requests response = requests . post( \"http://localhost:11235/crawl\" , headers = headers, json = { \"urls\" : \"https://example.com\" , \"priority\" : 10 } ) # Checking task status task_id = response . json()[ \"task_id\" ] status = requests . get( f\"http://localhost:11235/task/ { task_id } \" , headers = headers ) Using with Docker Compose # In your docker-compose.yml : services : crawl4ai : image : unclecode/crawl4ai:all environment : - CRAWL4AI_API_TOKEN=${CRAWL4AI_API_TOKEN:-} # Optional # ... other configuration Then either: 1. Set in .env file: CRAWL4AI_API_TOKEN=your_secret_token Or set via command line: CRAWL4AI_API_TOKEN = your_secret_token docker-compose up Security Note : If you enable the API token, make sure to keep it secure and never commit it to version control. The token will be required for all API endpoints except the health check endpoint ( /health ). Configuration Options \ud83d\udd27 # Environment Variables # You can configure the service using environment variables: # Basic configuration docker run -p 11235 :11235 \\ -e MAX_CONCURRENT_TASKS =5 \\ unclecode/crawl4ai:all # With security and LLM support docker run -p 11235 :11235 \\ -e CRAWL4AI_API_TOKEN = your_secret_token \\ -e OPENAI_API_KEY = sk-... \\ -e ANTHROPIC_API_KEY = sk-ant-... \\ unclecode/crawl4ai:all Using Docker Compose (Recommended) \ud83d\udc33 # Create a docker-compose.yml : version : '3.8' services : crawl4ai : image : unclecode/crawl4ai:all ports : - \"11235:11235\" environment : - CRAWL4AI_API_TOKEN=${CRAWL4AI_API_TOKEN:-} # Optional API security - MAX_CONCURRENT_TASKS=5 # LLM Provider Keys - OPENAI_API_KEY=${OPENAI_API_KEY:-} - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-} volumes : - /dev/shm:/dev/shm deploy : resources : limits : memory : 4G reservations : memory : 1G You can run it in two ways: Using environment variables directly: CRAWL4AI_API_TOKEN = secret123 OPENAI_API_KEY = sk-... docker-compose up Using a .env file (recommended): Create a .env file in the same directory: # API Security (optional) CRAWL4AI_API_TOKEN=your_secret_token # LLM Provider Keys OPENAI_API_KEY=sk-... ANTHROPIC_API_KEY=sk-ant-... # Other Configuration MAX_CONCURRENT_TASKS=5 Then simply run: docker-compose up Testing the Deployment \ud83e\uddea # import requests # For unsecured instances def test_unsecured (): # Health check health = requests . get( \"http://localhost:11235/health\" ) print ( \"Health check:\" , health . json()) # Basic crawl response = requests . post( \"http://localhost:11235/crawl\" , json = { \"urls\" : \"https://www.nbcnews.com/business\" , \"priority\" : 10 } ) task_id = response . json()[ \"task_id\" ] print ( \"Task ID:\" , task_id) # For secured instances def test_secured (api_token): headers = { \"Authorization\" : f\"Bearer { api_token } \" } # Basic crawl with authentication response = requests . post( \"http://localhost:11235/crawl\" , headers = headers, json = { \"urls\" : \"https://www.nbcnews.com/business\" , \"priority\" : 10 } ) task_id = response . json()[ \"task_id\" ] print ( \"Task ID:\" , task_id) LLM Extraction Example \ud83e\udd16 # When you've configured your LLM provider keys (via environment variables or .env ), you can use LLM extraction: request = { \"urls\" : \"https://example.com\" , \"extraction_config\" : { \"type\" : \"llm\" , \"params\" : { \"provider\" : \"openai/gpt-4\" , \"instruction\" : \"Extract main topics from the page\" } } } # Make the request (add headers if using API security) response = requests . post( \"http://localhost:11235/crawl\" , json = request) Note : Remember to add .env to your .gitignore to keep your API keys secure! Usage Examples \ud83d\udcdd # Basic Crawling # request = { \"urls\" : \"https://www.nbcnews.com/business\" , \"priority\" : 10 } response = requests . post( \"http://localhost:11235/crawl\" , json = request) task_id = response . json()[ \"task_id\" ] # Get results result = requests . get( f\"http://localhost:11235/task/ { task_id } \" ) Structured Data Extraction # schema = { \"name\" : \"Crypto Prices\" , \"baseSelector\" : \".cds-tableRow-t45thuk\" , \"fields\" : [ { \"name\" : \"crypto\" , \"selector\" : \"td:nth-child(1) h2\" , \"type\" : \"text\" , }, { \"name\" : \"price\" , \"selector\" : \"td:nth-child(2)\" , \"type\" : \"text\" , } ], } request = { \"urls\" : \"https://www.coinbase.com/explore\" , \"extraction_config\" : { \"type\" : \"json_css\" , \"params\" : { \"schema\" : schema} } } Dynamic Content Handling # request = { \"urls\" : \"https://www.nbcnews.com/business\" , \"js_code\" : [ \"const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();\" ], \"wait_for\" : \"article.tease-card:nth-child(10)\" } AI-Powered Extraction (Full Version) # request = { \"urls\" : \"https://www.nbcnews.com/business\" , \"extraction_config\" : { \"type\" : \"cosine\" , \"params\" : { \"semantic_filter\" : \"business finance economy\" , \"word_count_threshold\" : 10 , \"max_dist\" : 0.2 , \"top_k\" : 3 } } } Platform-Specific Instructions \ud83d\udcbb # macOS # docker pull unclecode/crawl4ai:basic docker run -p 11235 :11235 unclecode/crawl4ai:basic Ubuntu # # Basic version docker pull unclecode/crawl4ai:basic docker run -p 11235 :11235 unclecode/crawl4ai:basic # With GPU support docker pull unclecode/crawl4ai:gpu docker run --gpus all -p 11235 :11235 unclecode/crawl4ai:gpu Windows (PowerShell) # docker pull unclecode/crawl4ai:basic docker run -p 11235:11235 unclecode/crawl4ai:basic Testing \ud83e\uddea # Save this as test_docker.py : import requests import json import time import sys class Crawl4AiTester : def __init__ ( self , base_url: str = \"http://localhost:11235\" ): self . base_url = base_url def submit_and_wait ( self , request_data: dict , timeout: int = 300 ) -> dict : # Submit crawl job response = requests . post( f\" { self . base_url } /crawl\" , json = request_data) task_id = response . json()[ \"task_id\" ] print ( f\"Task ID: { task_id } \" ) # Poll for result start_time = time . time() while True : if time . time() - start_time > timeout: raise TimeoutError ( f\"Task { task_id } timeout\" ) result = requests . get( f\" { self . base_url } /task/ { task_id } \" ) status = result . json() if status[ \"status\" ] == \"completed\" : return status time . sleep( 2 ) def test_deployment (): tester = Crawl4AiTester() # Test basic crawl request = { \"urls\" : \"https://www.nbcnews.com/business\" , \"priority\" : 10 } result = tester . submit_and_wait(request) print ( \"Basic crawl successful!\" ) print ( f\"Content length: { len (result[ 'result' ][ 'markdown' ]) } \" ) if __name__ == \"__main__\" : test_deployment() Advanced Configuration \u2699\ufe0f # Crawler Parameters # The crawler_params field allows you to configure the browser instance and crawling behavior. Here are key parameters you can use: request = { \"urls\" : \"https://example.com\" , \"crawler_params\" : { # Browser Configuration \"headless\" : True , # Run in headless mode \"browser_type\" : \"chromium\" , # chromium/firefox/webkit \"user_agent\" : \"custom-agent\" , # Custom user agent \"proxy\" : \"http://proxy:8080\" , # Proxy configuration # Performance & Behavior \"page_timeout\" : 30000 , # Page load timeout (ms) \"verbose\" : True , # Enable detailed logging \"semaphore_count\" : 5 , # Concurrent request limit # Anti-Detection Features \"simulate_user\" : True , # Simulate human behavior \"magic\" : True , # Advanced anti-detection \"override_navigator\" : True , # Override navigator properties # Session Management \"user_data_dir\" : \"./browser-data\" , # Browser profile location \"use_managed_browser\" : True , # Use persistent browser } } Extra Parameters # The extra field allows passing additional parameters directly to the crawler's arun function: request = { \"urls\" : \"https://example.com\" , \"extra\" : { \"word_count_threshold\" : 10 , # Min words per block \"only_text\" : True , # Extract only text \"bypass_cache\" : True , # Force fresh crawl \"process_iframes\" : True , # Include iframe content } } Complete Examples # 1.\u2000 Advanced News Crawling request = { \"urls\" : \"https://www.nbcnews.com/business\" , \"crawler_params\" : { \"headless\" : True , \"page_timeout\" : 30000 , \"remove_overlay_elements\" : True # Remove popups }, \"extra\" : { \"word_count_threshold\" : 50 , # Longer content blocks \"bypass_cache\" : True # Fresh content }, \"css_selector\" : \".article-body\" } 2.\u2000 Anti-Detection Configuration request = { \"urls\" : \"https://example.com\" , \"crawler_params\" : { \"simulate_user\" : True , \"magic\" : True , \"override_navigator\" : True , \"user_agent\" : \"Mozilla/5.0 ...\" , \"headers\" : { \"Accept-Language\" : \"en-US,en;q=0.9\" } } } 3.\u2000 LLM Extraction with Custom Parameters request = { \"urls\" : \"https://openai.com/pricing\" , \"extraction_config\" : { \"type\" : \"llm\" , \"params\" : { \"provider\" : \"openai/gpt-4\" , \"schema\" : pricing_schema } }, \"crawler_params\" : { \"verbose\" : True , \"page_timeout\" : 60000 }, \"extra\" : { \"word_count_threshold\" : 1 , \"only_text\" : True } } 4.\u2000 Session-Based Dynamic Content request = { \"urls\" : \"https://example.com\" , \"crawler_params\" : { \"session_id\" : \"dynamic_session\" , \"headless\" : False , \"page_timeout\" : 60000 }, \"js_code\" : [ \"window.scrollTo(0, document.body.scrollHeight);\" ], \"wait_for\" : \"js:() => document.querySelectorAll('.item').length > 10\" , \"extra\" : { \"delay_before_return_html\" : 2.0 } } 5.\u2000 Screenshot with Custom Timing request = { \"urls\" : \"https://example.com\" , \"screenshot\" : True , \"crawler_params\" : { \"headless\" : True , \"screenshot_wait_for\" : \".main-content\" }, \"extra\" : { \"delay_before_return_html\" : 3.0 } } Parameter Reference Table # Category Parameter Type Description Browser headless bool Run browser in headless mode Browser browser_type str Browser engine selection Browser user_agent str Custom user agent string Network proxy str Proxy server URL Network headers dict Custom HTTP headers Timing page_timeout int Page load timeout (ms) Timing delay_before_return_html float Wait before capture Anti-Detection simulate_user bool Human behavior simulation Anti-Detection magic bool Advanced protection Session session_id str Browser session ID Session user_data_dir str Profile directory Content word_count_threshold int Minimum words per block Content only_text bool Text-only extraction Content process_iframes bool Include iframe content Debug verbose bool Detailed logging Debug log_console bool Browser console logs Troubleshooting \ud83d\udd0d # Common Issues # 1.\u2000 Connection Refused Error: Connection refused at localhost:11235 Solution: Ensure the container is running and ports are properly mapped. 2.\u2000 Resource Limits Error: No available slots Solution: Increase MAX_CONCURRENT_TASKS or container resources. 3.\u2000 GPU Access Error: GPU not found Solution: Ensure proper NVIDIA drivers and use --gpus all flag. Debug Mode # Access container for debugging: docker run -it --entrypoint /bin/bash unclecode/crawl4ai:all View container logs: docker logs [ container_id ] Best Practices \ud83c\udf1f # 1.\u2000 Resource Management - Set appropriate memory and CPU limits - Monitor resource usage via health endpoint - Use basic version for simple crawling tasks 2.\u2000 Scaling - Use multiple containers for high load - Implement proper load balancing - Monitor performance metrics 3.\u2000 Security - Use environment variables for sensitive data - Implement proper network isolation - Regular security updates API Reference \ud83d\udcda # Health Check # GET /health Submit Crawl Task # POST /crawl Content-Type: application/json { \"urls\": \"string or array\", \"extraction_config\": { \"type\": \"basic|llm|cosine|json_css\", \"params\": {} }, \"priority\": 1-10, \"ttl\": 3600 } Get Task Status # GET /task/{task_id} For more details, visit the official documentation .", "title": "Docker Deployment"}, {"location": "setup/docker-deployment/#docker-deployment", "text": "Crawl4AI provides official Docker images for easy deployment and scalability. This guide covers installation, configuration, and usage of Crawl4AI in Docker environments.", "title": "Docker Deployment"}, {"location": "setup/docker-deployment/#quick-start", "text": "Pull and run the basic version: # Basic run without security docker pull unclecode/crawl4ai:basic docker run -p 11235 :11235 unclecode/crawl4ai:basic # Run with API security enabled docker run -p 11235 :11235 -e CRAWL4AI_API_TOKEN = your_secret_token unclecode/crawl4ai:basic", "title": "Quick Start \ud83d\ude80"}, {"location": "setup/docker-deployment/#running-with-docker-compose", "text": "", "title": "Running with Docker Compose \ud83d\udc33"}, {"location": "setup/docker-deployment/#use-docker-compose-from-local-dockerfile-or-docker-hub", "text": "Crawl4AI provides flexibility to use Docker Compose for managing your containerized services. You can either build the image locally from the provided Dockerfile or use the pre-built image from Docker Hub.", "title": "Use Docker Compose (From Local Dockerfile or Docker Hub)"}, {"location": "setup/docker-deployment/#option-1-using-docker-compose-to-build-locally", "text": "If you want to build the image locally, use the provided docker-compose.local.yml file. docker-compose -f docker-compose.local.yml up -d This will: 1. Build the Docker image from the provided Dockerfile . 2. Start the container and expose it on http://localhost:11235 .", "title": "Option 1: Using Docker Compose to Build Locally"}, {"location": "setup/docker-deployment/#option-2-using-docker-compose-with-pre-built-image-from-hub", "text": "If you prefer using the pre-built image on Docker Hub, use the docker-compose.hub.yml file. docker-compose -f docker-compose.hub.yml up -d This will: 1. Pull the pre-built image unclecode/crawl4ai:basic (or all , depending on your configuration). 2. Start the container and expose it on http://localhost:11235 .", "title": "Option 2: Using Docker Compose with Pre-Built Image from Hub"}, {"location": "setup/docker-deployment/#stopping-the-running-services", "text": "To stop the services started via Docker Compose, you can use: docker-compose -f docker-compose.local.yml down # OR docker-compose -f docker-compose.hub.yml down If the containers don\u2019t stop and the application is still running, check the running containers: docker ps Find the CONTAINER ID of the running service and stop it forcefully: docker stop <CONTAINER_ID>", "title": "Stopping the Running Services"}, {"location": "setup/docker-deployment/#debugging-with-docker-compose", "text": "Check Logs : To view the container logs: docker-compose -f docker-compose.local.yml logs -f Remove Orphaned Containers : If the service is still running unexpectedly: docker-compose -f docker-compose.local.yml down --remove-orphans Manually Remove Network : If the network is still in use: docker network ls docker network rm crawl4ai_default", "title": "Debugging with Docker Compose"}, {"location": "setup/docker-deployment/#why-use-docker-compose", "text": "Docker Compose is the recommended way to deploy Crawl4AI because: 1. It simplifies multi-container setups. 2. Allows you to define environment variables, resources, and ports in a single file. 3. Makes it easier to switch between local development and production-ready images. For example, your docker-compose.yml could include API keys, token settings, and memory limits, making deployment quick and consistent.", "title": "Why Use Docker Compose?"}, {"location": "setup/docker-deployment/#api-security", "text": "", "title": "API Security \ud83d\udd12"}, {"location": "setup/docker-deployment/#understanding-crawl4ai_api_token", "text": "The CRAWL4AI_API_TOKEN provides optional security for your Crawl4AI instance: If CRAWL4AI_API_TOKEN is set: All API endpoints (except /health ) require authentication If CRAWL4AI_API_TOKEN is not set: The API is publicly accessible # Secured Instance docker run -p 11235 :11235 -e CRAWL4AI_API_TOKEN = your_secret_token unclecode/crawl4ai:all # Unsecured Instance docker run -p 11235 :11235 unclecode/crawl4ai:all", "title": "Understanding CRAWL4AI_API_TOKEN"}, {"location": "setup/docker-deployment/#making-api-calls", "text": "For secured instances, include the token in all requests: import requests # Setup headers if token is being used api_token = \"your_secret_token\" # Same token set in CRAWL4AI_API_TOKEN headers = { \"Authorization\" : f\"Bearer { api_token } \" } if api_token else {} # Making authenticated requests response = requests . post( \"http://localhost:11235/crawl\" , headers = headers, json = { \"urls\" : \"https://example.com\" , \"priority\" : 10 } ) # Checking task status task_id = response . json()[ \"task_id\" ] status = requests . get( f\"http://localhost:11235/task/ { task_id } \" , headers = headers )", "title": "Making API Calls"}, {"location": "setup/docker-deployment/#using-with-docker-compose", "text": "In your docker-compose.yml : services : crawl4ai : image : unclecode/crawl4ai:all environment : - CRAWL4AI_API_TOKEN=${CRAWL4AI_API_TOKEN:-} # Optional # ... other configuration Then either: 1. Set in .env file: CRAWL4AI_API_TOKEN=your_secret_token Or set via command line: CRAWL4AI_API_TOKEN = your_secret_token docker-compose up Security Note : If you enable the API token, make sure to keep it secure and never commit it to version control. The token will be required for all API endpoints except the health check endpoint ( /health ).", "title": "Using with Docker Compose"}, {"location": "setup/docker-deployment/#configuration-options", "text": "", "title": "Configuration Options \ud83d\udd27"}, {"location": "setup/docker-deployment/#environment-variables", "text": "You can configure the service using environment variables: # Basic configuration docker run -p 11235 :11235 \\ -e MAX_CONCURRENT_TASKS =5 \\ unclecode/crawl4ai:all # With security and LLM support docker run -p 11235 :11235 \\ -e CRAWL4AI_API_TOKEN = your_secret_token \\ -e OPENAI_API_KEY = sk-... \\ -e ANTHROPIC_API_KEY = sk-ant-... \\ unclecode/crawl4ai:all", "title": "Environment Variables"}, {"location": "setup/docker-deployment/#using-docker-compose-recommended", "text": "Create a docker-compose.yml : version : '3.8' services : crawl4ai : image : unclecode/crawl4ai:all ports : - \"11235:11235\" environment : - CRAWL4AI_API_TOKEN=${CRAWL4AI_API_TOKEN:-} # Optional API security - MAX_CONCURRENT_TASKS=5 # LLM Provider Keys - OPENAI_API_KEY=${OPENAI_API_KEY:-} - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-} volumes : - /dev/shm:/dev/shm deploy : resources : limits : memory : 4G reservations : memory : 1G You can run it in two ways: Using environment variables directly: CRAWL4AI_API_TOKEN = secret123 OPENAI_API_KEY = sk-... docker-compose up Using a .env file (recommended): Create a .env file in the same directory: # API Security (optional) CRAWL4AI_API_TOKEN=your_secret_token # LLM Provider Keys OPENAI_API_KEY=sk-... ANTHROPIC_API_KEY=sk-ant-... # Other Configuration MAX_CONCURRENT_TASKS=5 Then simply run: docker-compose up", "title": "Using Docker Compose (Recommended) \ud83d\udc33"}, {"location": "setup/docker-deployment/#testing-the-deployment", "text": "import requests # For unsecured instances def test_unsecured (): # Health check health = requests . get( \"http://localhost:11235/health\" ) print ( \"Health check:\" , health . json()) # Basic crawl response = requests . post( \"http://localhost:11235/crawl\" , json = { \"urls\" : \"https://www.nbcnews.com/business\" , \"priority\" : 10 } ) task_id = response . json()[ \"task_id\" ] print ( \"Task ID:\" , task_id) # For secured instances def test_secured (api_token): headers = { \"Authorization\" : f\"Bearer { api_token } \" } # Basic crawl with authentication response = requests . post( \"http://localhost:11235/crawl\" , headers = headers, json = { \"urls\" : \"https://www.nbcnews.com/business\" , \"priority\" : 10 } ) task_id = response . json()[ \"task_id\" ] print ( \"Task ID:\" , task_id)", "title": "Testing the Deployment \ud83e\uddea"}, {"location": "setup/docker-deployment/#llm-extraction-example", "text": "When you've configured your LLM provider keys (via environment variables or .env ), you can use LLM extraction: request = { \"urls\" : \"https://example.com\" , \"extraction_config\" : { \"type\" : \"llm\" , \"params\" : { \"provider\" : \"openai/gpt-4\" , \"instruction\" : \"Extract main topics from the page\" } } } # Make the request (add headers if using API security) response = requests . post( \"http://localhost:11235/crawl\" , json = request) Note : Remember to add .env to your .gitignore to keep your API keys secure!", "title": "LLM Extraction Example \ud83e\udd16"}, {"location": "setup/docker-deployment/#usage-examples", "text": "", "title": "Usage Examples \ud83d\udcdd"}, {"location": "setup/docker-deployment/#basic-crawling", "text": "request = { \"urls\" : \"https://www.nbcnews.com/business\" , \"priority\" : 10 } response = requests . post( \"http://localhost:11235/crawl\" , json = request) task_id = response . json()[ \"task_id\" ] # Get results result = requests . get( f\"http://localhost:11235/task/ { task_id } \" )", "title": "Basic Crawling"}, {"location": "setup/docker-deployment/#structured-data-extraction", "text": "schema = { \"name\" : \"Crypto Prices\" , \"baseSelector\" : \".cds-tableRow-t45thuk\" , \"fields\" : [ { \"name\" : \"crypto\" , \"selector\" : \"td:nth-child(1) h2\" , \"type\" : \"text\" , }, { \"name\" : \"price\" , \"selector\" : \"td:nth-child(2)\" , \"type\" : \"text\" , } ], } request = { \"urls\" : \"https://www.coinbase.com/explore\" , \"extraction_config\" : { \"type\" : \"json_css\" , \"params\" : { \"schema\" : schema} } }", "title": "Structured Data Extraction"}, {"location": "setup/docker-deployment/#dynamic-content-handling", "text": "request = { \"urls\" : \"https://www.nbcnews.com/business\" , \"js_code\" : [ \"const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();\" ], \"wait_for\" : \"article.tease-card:nth-child(10)\" }", "title": "Dynamic Content Handling"}, {"location": "setup/docker-deployment/#ai-powered-extraction-full-version", "text": "request = { \"urls\" : \"https://www.nbcnews.com/business\" , \"extraction_config\" : { \"type\" : \"cosine\" , \"params\" : { \"semantic_filter\" : \"business finance economy\" , \"word_count_threshold\" : 10 , \"max_dist\" : 0.2 , \"top_k\" : 3 } } }", "title": "AI-Powered Extraction (Full Version)"}, {"location": "setup/docker-deployment/#platform-specific-instructions", "text": "", "title": "Platform-Specific Instructions \ud83d\udcbb"}, {"location": "setup/docker-deployment/#macos", "text": "docker pull unclecode/crawl4ai:basic docker run -p 11235 :11235 unclecode/crawl4ai:basic", "title": "macOS"}, {"location": "setup/docker-deployment/#ubuntu", "text": "# Basic version docker pull unclecode/crawl4ai:basic docker run -p 11235 :11235 unclecode/crawl4ai:basic # With GPU support docker pull unclecode/crawl4ai:gpu docker run --gpus all -p 11235 :11235 unclecode/crawl4ai:gpu", "title": "Ubuntu"}, {"location": "setup/docker-deployment/#windows-powershell", "text": "docker pull unclecode/crawl4ai:basic docker run -p 11235:11235 unclecode/crawl4ai:basic", "title": "Windows (PowerShell)"}, {"location": "setup/docker-deployment/#testing", "text": "Save this as test_docker.py : import requests import json import time import sys class Crawl4AiTester : def __init__ ( self , base_url: str = \"http://localhost:11235\" ): self . base_url = base_url def submit_and_wait ( self , request_data: dict , timeout: int = 300 ) -> dict : # Submit crawl job response = requests . post( f\" { self . base_url } /crawl\" , json = request_data) task_id = response . json()[ \"task_id\" ] print ( f\"Task ID: { task_id } \" ) # Poll for result start_time = time . time() while True : if time . time() - start_time > timeout: raise TimeoutError ( f\"Task { task_id } timeout\" ) result = requests . get( f\" { self . base_url } /task/ { task_id } \" ) status = result . json() if status[ \"status\" ] == \"completed\" : return status time . sleep( 2 ) def test_deployment (): tester = Crawl4AiTester() # Test basic crawl request = { \"urls\" : \"https://www.nbcnews.com/business\" , \"priority\" : 10 } result = tester . submit_and_wait(request) print ( \"Basic crawl successful!\" ) print ( f\"Content length: { len (result[ 'result' ][ 'markdown' ]) } \" ) if __name__ == \"__main__\" : test_deployment()", "title": "Testing \ud83e\uddea"}, {"location": "setup/docker-deployment/#advanced-configuration", "text": "", "title": "Advanced Configuration \u2699\ufe0f"}, {"location": "setup/docker-deployment/#crawler-parameters", "text": "The crawler_params field allows you to configure the browser instance and crawling behavior. Here are key parameters you can use: request = { \"urls\" : \"https://example.com\" , \"crawler_params\" : { # Browser Configuration \"headless\" : True , # Run in headless mode \"browser_type\" : \"chromium\" , # chromium/firefox/webkit \"user_agent\" : \"custom-agent\" , # Custom user agent \"proxy\" : \"http://proxy:8080\" , # Proxy configuration # Performance & Behavior \"page_timeout\" : 30000 , # Page load timeout (ms) \"verbose\" : True , # Enable detailed logging \"semaphore_count\" : 5 , # Concurrent request limit # Anti-Detection Features \"simulate_user\" : True , # Simulate human behavior \"magic\" : True , # Advanced anti-detection \"override_navigator\" : True , # Override navigator properties # Session Management \"user_data_dir\" : \"./browser-data\" , # Browser profile location \"use_managed_browser\" : True , # Use persistent browser } }", "title": "Crawler Parameters"}, {"location": "setup/docker-deployment/#extra-parameters", "text": "The extra field allows passing additional parameters directly to the crawler's arun function: request = { \"urls\" : \"https://example.com\" , \"extra\" : { \"word_count_threshold\" : 10 , # Min words per block \"only_text\" : True , # Extract only text \"bypass_cache\" : True , # Force fresh crawl \"process_iframes\" : True , # Include iframe content } }", "title": "Extra Parameters"}, {"location": "setup/docker-deployment/#complete-examples", "text": "1.\u2000 Advanced News Crawling request = { \"urls\" : \"https://www.nbcnews.com/business\" , \"crawler_params\" : { \"headless\" : True , \"page_timeout\" : 30000 , \"remove_overlay_elements\" : True # Remove popups }, \"extra\" : { \"word_count_threshold\" : 50 , # Longer content blocks \"bypass_cache\" : True # Fresh content }, \"css_selector\" : \".article-body\" } 2.\u2000 Anti-Detection Configuration request = { \"urls\" : \"https://example.com\" , \"crawler_params\" : { \"simulate_user\" : True , \"magic\" : True , \"override_navigator\" : True , \"user_agent\" : \"Mozilla/5.0 ...\" , \"headers\" : { \"Accept-Language\" : \"en-US,en;q=0.9\" } } } 3.\u2000 LLM Extraction with Custom Parameters request = { \"urls\" : \"https://openai.com/pricing\" , \"extraction_config\" : { \"type\" : \"llm\" , \"params\" : { \"provider\" : \"openai/gpt-4\" , \"schema\" : pricing_schema } }, \"crawler_params\" : { \"verbose\" : True , \"page_timeout\" : 60000 }, \"extra\" : { \"word_count_threshold\" : 1 , \"only_text\" : True } } 4.\u2000 Session-Based Dynamic Content request = { \"urls\" : \"https://example.com\" , \"crawler_params\" : { \"session_id\" : \"dynamic_session\" , \"headless\" : False , \"page_timeout\" : 60000 }, \"js_code\" : [ \"window.scrollTo(0, document.body.scrollHeight);\" ], \"wait_for\" : \"js:() => document.querySelectorAll('.item').length > 10\" , \"extra\" : { \"delay_before_return_html\" : 2.0 } } 5.\u2000 Screenshot with Custom Timing request = { \"urls\" : \"https://example.com\" , \"screenshot\" : True , \"crawler_params\" : { \"headless\" : True , \"screenshot_wait_for\" : \".main-content\" }, \"extra\" : { \"delay_before_return_html\" : 3.0 } }", "title": "Complete Examples"}, {"location": "setup/docker-deployment/#parameter-reference-table", "text": "Category Parameter Type Description Browser headless bool Run browser in headless mode Browser browser_type str Browser engine selection Browser user_agent str Custom user agent string Network proxy str Proxy server URL Network headers dict Custom HTTP headers Timing page_timeout int Page load timeout (ms) Timing delay_before_return_html float Wait before capture Anti-Detection simulate_user bool Human behavior simulation Anti-Detection magic bool Advanced protection Session session_id str Browser session ID Session user_data_dir str Profile directory Content word_count_threshold int Minimum words per block Content only_text bool Text-only extraction Content process_iframes bool Include iframe content Debug verbose bool Detailed logging Debug log_console bool Browser console logs", "title": "Parameter Reference Table"}, {"location": "setup/docker-deployment/#troubleshooting", "text": "", "title": "Troubleshooting \ud83d\udd0d"}, {"location": "setup/docker-deployment/#common-issues", "text": "1.\u2000 Connection Refused Error: Connection refused at localhost:11235 Solution: Ensure the container is running and ports are properly mapped. 2.\u2000 Resource Limits Error: No available slots Solution: Increase MAX_CONCURRENT_TASKS or container resources. 3.\u2000 GPU Access Error: GPU not found Solution: Ensure proper NVIDIA drivers and use --gpus all flag.", "title": "Common Issues"}, {"location": "setup/docker-deployment/#debug-mode", "text": "Access container for debugging: docker run -it --entrypoint /bin/bash unclecode/crawl4ai:all View container logs: docker logs [ container_id ]", "title": "Debug Mode"}, {"location": "setup/docker-deployment/#best-practices", "text": "1.\u2000 Resource Management - Set appropriate memory and CPU limits - Monitor resource usage via health endpoint - Use basic version for simple crawling tasks 2.\u2000 Scaling - Use multiple containers for high load - Implement proper load balancing - Monitor performance metrics 3.\u2000 Security - Use environment variables for sensitive data - Implement proper network isolation - Regular security updates", "title": "Best Practices \ud83c\udf1f"}, {"location": "setup/docker-deployment/#api-reference", "text": "", "title": "API Reference \ud83d\udcda"}, {"location": "setup/docker-deployment/#health-check", "text": "GET /health", "title": "Health Check"}, {"location": "setup/docker-deployment/#submit-crawl-task", "text": "POST /crawl Content-Type: application/json { \"urls\": \"string or array\", \"extraction_config\": { \"type\": \"basic|llm|cosine|json_css\", \"params\": {} }, \"priority\": 1-10, \"ttl\": 3600 }", "title": "Submit Crawl Task"}, {"location": "setup/docker-deployment/#get-task-status", "text": "GET /task/{task_id} For more details, visit the official documentation .", "title": "Get Task Status"}, {"location": "setup/installation/", "text": "Getting Started with Crawl4AI # Welcome to Crawl4AI , an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, you\u2019ll: Run your first crawl using minimal configuration. Generate Markdown output (and learn how it\u2019s influenced by content filters). Experiment with a simple CSS-based extraction strategy. See a glimpse of LLM-based extraction (including open-source and closed-source model options). Crawl a dynamic page that loads content via JavaScript. 1. Introduction # Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler . Configurable browser and run settings via BrowserConfig and CrawlerRunConfig . Automatic HTML-to-Markdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLM-based or \u201ctraditional\u201d CSS/XPath-based). By the end of this guide, you\u2019ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses \u201cLoad More\u201d buttons or JavaScript updates. 2. Your First Crawl # Here\u2019s a minimal Python script that creates an AsyncWebCrawler , fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main (): async with AsyncWebCrawler() as crawler: result = await crawler . arun( \"https://example.com\" ) print (result . markdown[: 300 ]) # Print first 300 chars if __name__ == \"__main__\" : asyncio . run(main()) What\u2019s happening? - AsyncWebCrawler launches a headless browser (Chromium by default). - It fetches https://example.com . - Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl! 3. Basic Configuration (Light Introduction) # Crawl4AI\u2019s crawler can be heavily customized using two main classes: 1.\u2000 BrowserConfig : Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2.\u2000 CrawlerRunConfig : Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main (): browser_conf = BrowserConfig(headless = True ) # or False to see the browser run_conf = CrawlerRunConfig( cache_mode = CacheMode . BYPASS ) async with AsyncWebCrawler(config = browser_conf) as crawler: result = await crawler . arun( url = \"https://example.com\" , config = run_conf ) print (result . markdown) if __name__ == \"__main__\" : asyncio . run(main()) IMPORTANT: By default cache mode is set to CacheMode.ENABLED . So to have fresh content, you need to set it to CacheMode.BYPASS We\u2019ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling. 4. Generating Markdown Output # By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter . result.markdown : The direct HTML-to-Markdown conversion. result.markdown.fit_markdown : The same content after applying any configured content filter (e.g., PruningContentFilter ). Example: Using a Filter with DefaultMarkdownGenerator # from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.content_filter_strategy import PruningContentFilter from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator md_generator = DefaultMarkdownGenerator( content_filter = PruningContentFilter(threshold =0.4 , threshold_type = \"fixed\" ) ) config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, markdown_generator = md_generator ) async with AsyncWebCrawler() as crawler: result = await crawler . arun( \"https://news.ycombinator.com\" , config = config) print ( \"Raw Markdown length:\" , len (result . markdown . raw_markdown)) print ( \"Fit Markdown length:\" , len (result . markdown . fit_markdown)) Note : If you do not specify a content filter or markdown generator, you\u2019ll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. We\u2019ll dive deeper into these strategies in a dedicated Markdown Generation tutorial. 5. Simple Data Extraction (CSS-based) # Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example: New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions: from crawl4ai.extraction_strategy import JsonCssExtractionStrategy from crawl4ai.types import LLMConfig # Generate a schema (one-time cost) html = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\" # Using OpenAI (requires API token) schema = JsonCssExtractionStrategy . generate_schema( html, llm_config = LLMConfig(provider = \"openai/gpt-4o\" ,api_token = \"your-openai-token\" ) # Required for OpenAI ) # Or using Ollama (open source, no token needed) schema = JsonCssExtractionStrategy . generate_schema( html, llm_config = LLMConfig(provider = \"ollama/llama3.3\" , api_token = None ) # Not needed for Ollama ) # Use the schema for fast, repeated extractions strategy = JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see No-LLM Extraction Strategies . Here's a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def main (): schema = { \"name\" : \"Example Items\" , \"baseSelector\" : \"div.item\" , \"fields\" : [ { \"name\" : \"title\" , \"selector\" : \"h2\" , \"type\" : \"text\" }, { \"name\" : \"link\" , \"selector\" : \"a\" , \"type\" : \"attribute\" , \"attribute\" : \"href\" } ] } raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\" async with AsyncWebCrawler() as crawler: result = await crawler . arun( url = \"raw://\" + raw_html, config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, extraction_strategy = JsonCssExtractionStrategy(schema) ) ) # The JSON output is stored in 'extracted_content' data = json . loads(result . extracted_content) print (data) if __name__ == \"__main__\" : asyncio . run(main()) Why is this helpful? - Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store. Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw:// . 6. Simple Data Extraction (LLM-based) # For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports open-source or closed-source providers: Open-Source Models (e.g., ollama/llama3.3 , no_token ) OpenAI Models (e.g., openai/gpt-4 , requires api_token ) Or any provider supported by the underlying library Below is an example using open-source style (no token) and closed-source: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig from crawl4ai.extraction_strategy import LLMExtractionStrategy class OpenAIModelFee (BaseModel): model_name: str = Field( ... , description = \"Name of the OpenAI model.\" ) input_fee: str = Field( ... , description = \"Fee for input token for the OpenAI model.\" ) output_fee: str = Field( ... , description = \"Fee for output token for the OpenAI model.\" ) async def extract_structured_data_using_llm ( provider: str , api_token: str = None , extra_headers: Dict[ str , str ] = None ): print ( f\" \\n --- Extracting Structured Data with { provider } ---\" ) if api_token is None and provider != \"ollama\" : print ( f\"API token is required for { provider } . Skipping this example.\" ) return browser_config = BrowserConfig(headless = True ) extra_args = { \"temperature\" : 0 , \"top_p\" : 0.9 , \"max_tokens\" : 2000 } if extra_headers: extra_args[ \"extra_headers\" ] = extra_headers crawler_config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, word_count_threshold =1 , page_timeout =80000 , extraction_strategy = LLMExtractionStrategy( llm_config = LLMConfig(provider = provider,api_token = api_token), schema = OpenAIModelFee . model_json_schema(), extraction_type = \"schema\" , instruction = \"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content.\"\"\" , extra_args = extra_args, ), ) async with AsyncWebCrawler(config = browser_config) as crawler: result = await crawler . arun( url = \"https://openai.com/api/pricing/\" , config = crawler_config ) print (result . extracted_content) if __name__ == \"__main__\" : asyncio . run( extract_structured_data_using_llm( provider = \"openai/gpt-4o\" , api_token = os . getenv( \"OPENAI_API_KEY\" ) ) ) What\u2019s happening? - We define a Pydantic schema ( PricingInfo ) describing the fields we want. - The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON. - Depending on the provider and api_token , you can use local models or a remote API. 7. Multi-URL Concurrency (Preview) # If you need to crawl multiple URLs in parallel , you can use arun_many() . By default, Crawl4AI employs a MemoryAdaptiveDispatcher , automatically adjusting concurrency based on system resources. Here\u2019s a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quick_parallel_example (): urls = [ \"https://example.com/page1\" , \"https://example.com/page2\" , \"https://example.com/page3\" ] run_conf = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, stream = True # Enable streaming mode ) async with AsyncWebCrawler() as crawler: # Stream results as they complete async for result in await crawler . arun_many(urls, config = run_conf): if result . success: print ( f\"[OK] { result . url } , length: { len (result . markdown . raw_markdown) } \" ) else : print ( f\"[ERROR] { result . url } => { result . error_message } \" ) # Or get all results at once (default behavior) run_conf = run_conf . clone(stream = False ) results = await crawler . arun_many(urls, config = run_conf) for res in results: if res . success: print ( f\"[OK] { res . url } , length: { len (res . markdown . raw_markdown) } \" ) else : print ( f\"[ERROR] { res . url } => { res . error_message } \" ) if __name__ == \"__main__\" : asyncio . run(quick_parallel_example()) The example above shows two ways to handle multiple URLs: 1. Streaming mode ( stream=True ): Process results as they become available using async for 2. Batch mode ( stream=False ): Wait for all results to complete For more advanced concurrency (e.g., a semaphore-based approach, adaptive memory usage throttling , or customized rate limiting), see Advanced Multi-URL Crawling . 8. Dynamic Content Example # Some sites require multiple \u201cpage clicks\u201d or dynamic JavaScript updates. Below is an example showing how to click a \u201cNext Page\u201d button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig : import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def extract_structured_data_using_css_extractor (): print ( \" \\n --- Using JsonCssExtractionStrategy for Fast Structured Output ---\" ) schema = { \"name\" : \"KidoCode Courses\" , \"baseSelector\" : \"section.charge-methodology .w-tab-content > div\" , \"fields\" : [ { \"name\" : \"section_title\" , \"selector\" : \"h3.heading-50\" , \"type\" : \"text\" , }, { \"name\" : \"section_description\" , \"selector\" : \".charge-content\" , \"type\" : \"text\" , }, { \"name\" : \"course_name\" , \"selector\" : \".text-block-93\" , \"type\" : \"text\" , }, { \"name\" : \"course_description\" , \"selector\" : \".course-content-text\" , \"type\" : \"text\" , }, { \"name\" : \"course_icon\" , \"selector\" : \".image-92\" , \"type\" : \"attribute\" , \"attribute\" : \"src\" , }, ], } browser_config = BrowserConfig(headless = True , java_script_enabled = True ) js_click_tabs = \"\"\" (async () => { const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\"); for(let tab of tabs) { tab.scrollIntoView(); tab.click(); await new Promise(r => setTimeout(r, 500)); } })(); \"\"\" crawler_config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, extraction_strategy = JsonCssExtractionStrategy(schema), js_code = [js_click_tabs], ) async with AsyncWebCrawler(config = browser_config) as crawler: result = await crawler . arun( url = \"https://www.kidocode.com/degrees/technology\" , config = crawler_config ) companies = json . loads(result . extracted_content) print ( f\"Successfully extracted { len (companies) } companies\" ) print (json . dumps(companies[ 0 ], indent =2 )) async def main (): await extract_structured_data_using_css_extractor() if __name__ == \"__main__\" : asyncio . run(main()) Key Points : BrowserConfig(headless=False) : We want to watch it click \u201cNext Page.\u201d CrawlerRunConfig(...) : We specify the extraction strategy, pass session_id to reuse the same page. js_code and wait_for are used for subsequent pages ( page > 0 ) to click the \u201cNext\u201d button and wait for new commits to load. js_only=True indicates we\u2019re not re-navigating but continuing the existing session. Finally, we call kill_session() to clean up the page and browser session. 9. Next Steps # Congratulations! You have: Performed a basic crawl and printed Markdown. Used content filters with a markdown generator. Extracted JSON via CSS or LLM strategies. Handled dynamic pages with JavaScript triggers. If you\u2019re ready for more, check out: Installation : A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks & Auth : Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment : Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management : Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling!", "title": "Installation"}, {"location": "setup/installation/#getting-started-with-crawl4ai", "text": "Welcome to Crawl4AI , an open-source LLM-friendly Web Crawler & Scraper. In this tutorial, you\u2019ll: Run your first crawl using minimal configuration. Generate Markdown output (and learn how it\u2019s influenced by content filters). Experiment with a simple CSS-based extraction strategy. See a glimpse of LLM-based extraction (including open-source and closed-source model options). Crawl a dynamic page that loads content via JavaScript.", "title": "Getting Started with Crawl4AI"}, {"location": "setup/installation/#1-introduction", "text": "Crawl4AI provides: An asynchronous crawler, AsyncWebCrawler . Configurable browser and run settings via BrowserConfig and CrawlerRunConfig . Automatic HTML-to-Markdown conversion via DefaultMarkdownGenerator (supports optional filters). Multiple extraction strategies (LLM-based or \u201ctraditional\u201d CSS/XPath-based). By the end of this guide, you\u2019ll have performed a basic crawl, generated Markdown, tried out two extraction strategies, and crawled a dynamic page that uses \u201cLoad More\u201d buttons or JavaScript updates.", "title": "1. Introduction"}, {"location": "setup/installation/#2-your-first-crawl", "text": "Here\u2019s a minimal Python script that creates an AsyncWebCrawler , fetches a webpage, and prints the first 300 characters of its Markdown output: import asyncio from crawl4ai import AsyncWebCrawler async def main (): async with AsyncWebCrawler() as crawler: result = await crawler . arun( \"https://example.com\" ) print (result . markdown[: 300 ]) # Print first 300 chars if __name__ == \"__main__\" : asyncio . run(main()) What\u2019s happening? - AsyncWebCrawler launches a headless browser (Chromium by default). - It fetches https://example.com . - Crawl4AI automatically converts the HTML into Markdown. You now have a simple, working crawl!", "title": "2. Your First Crawl"}, {"location": "setup/installation/#3-basic-configuration-light-introduction", "text": "Crawl4AI\u2019s crawler can be heavily customized using two main classes: 1.\u2000 BrowserConfig : Controls browser behavior (headless or full UI, user agent, JavaScript toggles, etc.). 2.\u2000 CrawlerRunConfig : Controls how each crawl runs (caching, extraction, timeouts, hooking, etc.). Below is an example with minimal usage: import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode async def main (): browser_conf = BrowserConfig(headless = True ) # or False to see the browser run_conf = CrawlerRunConfig( cache_mode = CacheMode . BYPASS ) async with AsyncWebCrawler(config = browser_conf) as crawler: result = await crawler . arun( url = \"https://example.com\" , config = run_conf ) print (result . markdown) if __name__ == \"__main__\" : asyncio . run(main()) IMPORTANT: By default cache mode is set to CacheMode.ENABLED . So to have fresh content, you need to set it to CacheMode.BYPASS We\u2019ll explore more advanced config in later tutorials (like enabling proxies, PDF output, multi-tab sessions, etc.). For now, just note how you pass these objects to manage crawling.", "title": "3. Basic Configuration (Light Introduction)"}, {"location": "setup/installation/#4-generating-markdown-output", "text": "By default, Crawl4AI automatically generates Markdown from each crawled page. However, the exact output depends on whether you specify a markdown generator or content filter . result.markdown : The direct HTML-to-Markdown conversion. result.markdown.fit_markdown : The same content after applying any configured content filter (e.g., PruningContentFilter ).", "title": "4. Generating Markdown Output"}, {"location": "setup/installation/#example-using-a-filter-with-defaultmarkdowngenerator", "text": "from crawl4ai import AsyncWebCrawler, CrawlerRunConfig from crawl4ai.content_filter_strategy import PruningContentFilter from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator md_generator = DefaultMarkdownGenerator( content_filter = PruningContentFilter(threshold =0.4 , threshold_type = \"fixed\" ) ) config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, markdown_generator = md_generator ) async with AsyncWebCrawler() as crawler: result = await crawler . arun( \"https://news.ycombinator.com\" , config = config) print ( \"Raw Markdown length:\" , len (result . markdown . raw_markdown)) print ( \"Fit Markdown length:\" , len (result . markdown . fit_markdown)) Note : If you do not specify a content filter or markdown generator, you\u2019ll typically see only the raw Markdown. PruningContentFilter may adds around 50ms in processing time. We\u2019ll dive deeper into these strategies in a dedicated Markdown Generation tutorial.", "title": "Example: Using a Filter with DefaultMarkdownGenerator"}, {"location": "setup/installation/#5-simple-data-extraction-css-based", "text": "Crawl4AI can also extract structured data (JSON) using CSS or XPath selectors. Below is a minimal CSS-based example: New! Crawl4AI now provides a powerful utility to automatically generate extraction schemas using LLM. This is a one-time cost that gives you a reusable schema for fast, LLM-free extractions: from crawl4ai.extraction_strategy import JsonCssExtractionStrategy from crawl4ai.types import LLMConfig # Generate a schema (one-time cost) html = \"<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>\" # Using OpenAI (requires API token) schema = JsonCssExtractionStrategy . generate_schema( html, llm_config = LLMConfig(provider = \"openai/gpt-4o\" ,api_token = \"your-openai-token\" ) # Required for OpenAI ) # Or using Ollama (open source, no token needed) schema = JsonCssExtractionStrategy . generate_schema( html, llm_config = LLMConfig(provider = \"ollama/llama3.3\" , api_token = None ) # Not needed for Ollama ) # Use the schema for fast, repeated extractions strategy = JsonCssExtractionStrategy(schema) For a complete guide on schema generation and advanced usage, see No-LLM Extraction Strategies . Here's a basic extraction example: import asyncio import json from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def main (): schema = { \"name\" : \"Example Items\" , \"baseSelector\" : \"div.item\" , \"fields\" : [ { \"name\" : \"title\" , \"selector\" : \"h2\" , \"type\" : \"text\" }, { \"name\" : \"link\" , \"selector\" : \"a\" , \"type\" : \"attribute\" , \"attribute\" : \"href\" } ] } raw_html = \"<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>\" async with AsyncWebCrawler() as crawler: result = await crawler . arun( url = \"raw://\" + raw_html, config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, extraction_strategy = JsonCssExtractionStrategy(schema) ) ) # The JSON output is stored in 'extracted_content' data = json . loads(result . extracted_content) print (data) if __name__ == \"__main__\" : asyncio . run(main()) Why is this helpful? - Great for repetitive page structures (e.g., item listings, articles). - No AI usage or costs. - The crawler returns a JSON string you can parse or store. Tips: You can pass raw HTML to the crawler instead of a URL. To do so, prefix the HTML with raw:// .", "title": "5. Simple Data Extraction (CSS-based)"}, {"location": "setup/installation/#6-simple-data-extraction-llm-based", "text": "For more complex or irregular pages, a language model can parse text intelligently into a structure you define. Crawl4AI supports open-source or closed-source providers: Open-Source Models (e.g., ollama/llama3.3 , no_token ) OpenAI Models (e.g., openai/gpt-4 , requires api_token ) Or any provider supported by the underlying library Below is an example using open-source style (no token) and closed-source: import os import json import asyncio from pydantic import BaseModel, Field from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig from crawl4ai.extraction_strategy import LLMExtractionStrategy class OpenAIModelFee (BaseModel): model_name: str = Field( ... , description = \"Name of the OpenAI model.\" ) input_fee: str = Field( ... , description = \"Fee for input token for the OpenAI model.\" ) output_fee: str = Field( ... , description = \"Fee for output token for the OpenAI model.\" ) async def extract_structured_data_using_llm ( provider: str , api_token: str = None , extra_headers: Dict[ str , str ] = None ): print ( f\" \\n --- Extracting Structured Data with { provider } ---\" ) if api_token is None and provider != \"ollama\" : print ( f\"API token is required for { provider } . Skipping this example.\" ) return browser_config = BrowserConfig(headless = True ) extra_args = { \"temperature\" : 0 , \"top_p\" : 0.9 , \"max_tokens\" : 2000 } if extra_headers: extra_args[ \"extra_headers\" ] = extra_headers crawler_config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, word_count_threshold =1 , page_timeout =80000 , extraction_strategy = LLMExtractionStrategy( llm_config = LLMConfig(provider = provider,api_token = api_token), schema = OpenAIModelFee . model_json_schema(), extraction_type = \"schema\" , instruction = \"\"\"From the crawled content, extract all mentioned model names along with their fees for input and output tokens. Do not miss any models in the entire content.\"\"\" , extra_args = extra_args, ), ) async with AsyncWebCrawler(config = browser_config) as crawler: result = await crawler . arun( url = \"https://openai.com/api/pricing/\" , config = crawler_config ) print (result . extracted_content) if __name__ == \"__main__\" : asyncio . run( extract_structured_data_using_llm( provider = \"openai/gpt-4o\" , api_token = os . getenv( \"OPENAI_API_KEY\" ) ) ) What\u2019s happening? - We define a Pydantic schema ( PricingInfo ) describing the fields we want. - The LLM extraction strategy uses that schema and your instructions to transform raw text into structured JSON. - Depending on the provider and api_token , you can use local models or a remote API.", "title": "6. Simple Data Extraction (LLM-based)"}, {"location": "setup/installation/#7-multi-url-concurrency-preview", "text": "If you need to crawl multiple URLs in parallel , you can use arun_many() . By default, Crawl4AI employs a MemoryAdaptiveDispatcher , automatically adjusting concurrency based on system resources. Here\u2019s a quick glimpse: import asyncio from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode async def quick_parallel_example (): urls = [ \"https://example.com/page1\" , \"https://example.com/page2\" , \"https://example.com/page3\" ] run_conf = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, stream = True # Enable streaming mode ) async with AsyncWebCrawler() as crawler: # Stream results as they complete async for result in await crawler . arun_many(urls, config = run_conf): if result . success: print ( f\"[OK] { result . url } , length: { len (result . markdown . raw_markdown) } \" ) else : print ( f\"[ERROR] { result . url } => { result . error_message } \" ) # Or get all results at once (default behavior) run_conf = run_conf . clone(stream = False ) results = await crawler . arun_many(urls, config = run_conf) for res in results: if res . success: print ( f\"[OK] { res . url } , length: { len (res . markdown . raw_markdown) } \" ) else : print ( f\"[ERROR] { res . url } => { res . error_message } \" ) if __name__ == \"__main__\" : asyncio . run(quick_parallel_example()) The example above shows two ways to handle multiple URLs: 1. Streaming mode ( stream=True ): Process results as they become available using async for 2. Batch mode ( stream=False ): Wait for all results to complete For more advanced concurrency (e.g., a semaphore-based approach, adaptive memory usage throttling , or customized rate limiting), see Advanced Multi-URL Crawling .", "title": "7. Multi-URL Concurrency (Preview)"}, {"location": "setup/installation/#8-dynamic-content-example", "text": "Some sites require multiple \u201cpage clicks\u201d or dynamic JavaScript updates. Below is an example showing how to click a \u201cNext Page\u201d button and wait for new commits to load on GitHub, using BrowserConfig and CrawlerRunConfig : import asyncio from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode from crawl4ai.extraction_strategy import JsonCssExtractionStrategy async def extract_structured_data_using_css_extractor (): print ( \" \\n --- Using JsonCssExtractionStrategy for Fast Structured Output ---\" ) schema = { \"name\" : \"KidoCode Courses\" , \"baseSelector\" : \"section.charge-methodology .w-tab-content > div\" , \"fields\" : [ { \"name\" : \"section_title\" , \"selector\" : \"h3.heading-50\" , \"type\" : \"text\" , }, { \"name\" : \"section_description\" , \"selector\" : \".charge-content\" , \"type\" : \"text\" , }, { \"name\" : \"course_name\" , \"selector\" : \".text-block-93\" , \"type\" : \"text\" , }, { \"name\" : \"course_description\" , \"selector\" : \".course-content-text\" , \"type\" : \"text\" , }, { \"name\" : \"course_icon\" , \"selector\" : \".image-92\" , \"type\" : \"attribute\" , \"attribute\" : \"src\" , }, ], } browser_config = BrowserConfig(headless = True , java_script_enabled = True ) js_click_tabs = \"\"\" (async () => { const tabs = document.querySelectorAll(\"section.charge-methodology .tabs-menu-3 > div\"); for(let tab of tabs) { tab.scrollIntoView(); tab.click(); await new Promise(r => setTimeout(r, 500)); } })(); \"\"\" crawler_config = CrawlerRunConfig( cache_mode = CacheMode . BYPASS, extraction_strategy = JsonCssExtractionStrategy(schema), js_code = [js_click_tabs], ) async with AsyncWebCrawler(config = browser_config) as crawler: result = await crawler . arun( url = \"https://www.kidocode.com/degrees/technology\" , config = crawler_config ) companies = json . loads(result . extracted_content) print ( f\"Successfully extracted { len (companies) } companies\" ) print (json . dumps(companies[ 0 ], indent =2 )) async def main (): await extract_structured_data_using_css_extractor() if __name__ == \"__main__\" : asyncio . run(main()) Key Points : BrowserConfig(headless=False) : We want to watch it click \u201cNext Page.\u201d CrawlerRunConfig(...) : We specify the extraction strategy, pass session_id to reuse the same page. js_code and wait_for are used for subsequent pages ( page > 0 ) to click the \u201cNext\u201d button and wait for new commits to load. js_only=True indicates we\u2019re not re-navigating but continuing the existing session. Finally, we call kill_session() to clean up the page and browser session.", "title": "8. Dynamic Content Example"}, {"location": "setup/installation/#9-next-steps", "text": "Congratulations! You have: Performed a basic crawl and printed Markdown. Used content filters with a markdown generator. Extracted JSON via CSS or LLM strategies. Handled dynamic pages with JavaScript triggers. If you\u2019re ready for more, check out: Installation : A deeper dive into advanced installs, Docker usage (experimental), or optional dependencies. Hooks & Auth : Learn how to run custom JavaScript or handle logins with cookies, local storage, etc. Deployment : Explore ephemeral testing in Docker or plan for the upcoming stable Docker release. Browser Management : Delve into user simulation, stealth modes, and concurrency best practices. Crawl4AI is a powerful, flexible tool. Enjoy building out your scrapers, data pipelines, or AI-driven extraction flows. Happy crawling!", "title": "9. Next Steps"}]}